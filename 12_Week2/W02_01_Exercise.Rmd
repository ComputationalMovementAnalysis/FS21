---
title: "Week2"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = "/home/rata/CMA_FS2018")

```



## Tasks

### Task 1: Handling dates and times

We are used to storing dates and times in individual columns from Microsoft Excel. Superficially, this is sufficant to do time series analysis. As analysis get's more complex, you will realize that this is not enough. We therefore store our date/time information in *one* column together with the according timezohne. There are two types of datetime classes in R, we will use POSIXct. In this task, concatenate date and time (using paste()) and assign the class POSIXct (using as.POSIXct or readr::parse_datetime()). 

Whichever method you're using, R will need to know what format your values are stored in. In our dataframe, the date is stored in the format "Day.Month.Year" and the Time is stored as "Hours:Minutes:Seconds". "Day.Month.Year" translates to "%d.%m.%Y" and "Hours:Minutes:Seconds" translates to %H:%M:%S in R. Specify this using the option "format = ". Store the new vector in a variable (e.g. datetime_POSIXct)
```{r}

# Concatenating the two columns into one using paste()
datetime_paste <- paste(roe_gps_all$LMT_Date,roe_gps_all$LMT_Time)

# paste() just generates character vectors again
str(datetime_paste)

# we therefore have to turn the character vector into a datetime class. 
datetime_POSIXct <- parse_datetime(datetime_paste,format = "%d.%m.%Y %H:%M:%S")
```


Look at your new vector datetime_POSIXct. What Time zone did R assume? 

```{r}

str(datetime_POSIXct)

head(datetime_POSIXct)

```


Compare this time zone with the column name: Did R assume the time zone correctly? It is error-prone to let R simply assume a Time zone. Therefore, it is better to explicitly specify a time zone. Do this in R. 
```{r}
datetime_POSIXct <- parse_datetime(datetime_paste,format = "%d.%m.%Y %H:%M:%S", locale = locale(tz = "Europe/Berlin"))
```

Once you have a POSIXct you can _extract_ information from it. We recommend using lubridate for this. Lubridate is installed when installing tidyverse, but oddly enough it isn't loaded when calling library(lubridate). Call lubridate now and try out the functions month(), year() and so on. Filter the data for July 2014.

```{r}
library(lubridate)
```


### Task 2

#############################################################################
## Joining data #############################################################
#############################################################################

We have a table with additional data about our animals. If we want to add
this data (e.g. sex and weight) to our data.frame automatically, we need to 
join the data by a common variable, e.g., the animals' ID. 

But first, import the data with the roe deer information into the project 
"Rawdata_import.R" using the following command:
roe_meta <- read.delim("Rawdata/Roe_meta.csv", header = T, sep = ";")

There are several ways to join columns using base-R functions. We will, however, 
use a function provided by the package dplyr, since we will be using this 
package for a lot of tasks during the course. Import the dplyr library into your 
project now.

We want to add all values from the table "roe_meta.csv" (stored in the variable 
"roe_meta") to the data.frame "gps_roe_all" via the animal ID. Note that 
the column-names are different for the two ID columns ("TierID" and "ID").

```{r}
gps_roe_all <- left_join(gps_roe_all, roe_meta, by = c("TierID" = "ID"))

```

You probably recieved an warning message from dplyr, saying that the joining factors
have different factor levels. This is because our "roe_meta"-data has an animal ID
which is not available in the gps_roe_all data (RE13). Since I want to ignore this
animal in my join, I chose a "left join" operation.  If you are not familiar with 
the different type of joins, study the following image:
http://planspace.org/20150530-practical_mergic_at_odsc/img/dplyr_joins.png

```{r}
head(gps_roe_all)
```



#############################################################################
#############################################################################
#############################################################################
#############################################################################
#############################################################################
#############################################################################


#############################################################################
#An introduction to SAC: Split-Apply-Combine ##############################
#############################################################################

When working with data, many tasks involve splitting the data into different
categories, proccessing these subcategories in a specific way, and then 
re-combining the data. This is know as the SAC-paradigm
(split-apply-combine).

Base-R has some built-in functions to take care of SAC tasks. The dplyr package
(which we got to know in the join operation) is much more powerful and we will 
be using dplyr to address SAC tasks.

The DOP Value in our gps data shows the dilution of precision, i.e. gives an  
indication of how precise the gps-point was captured by the satellites. For more 
information on DOP, check:
https://en.wikipedia.org/wiki/Dilution_of_precision_%28GPS%29

Let's say we want to get some information about the difference of DOP values
between the different animals. We could use something like this:
mean(gps_roe_all$DOP[gps_roe_all$TierID == "RE02"])
but that would be very tedious, especially if you have many different animals

Instead, let's use dplyr. With dplyr, we will need to group our data first (the
S in our SAC-paradigm). This is done like this:
gps_roe_grouped <- group_by(gps_roe_all, TierID)

colnames(gps_roe_grouped)
Read more about group_by here:
https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html

next, you can start summarising. 
summarise(gps_roe_grouped, mean = mean(DOP), max = max(DOP), min = min(DOP))
We took care of SPLITTING our data with the "group_by()" command. the functions
"mean()", "max()" and "min()" are APPLIED to our SPLIT data and "summarise()"
COMBINES our data again (SAC).

The output is a short table summarising our original data. If you want to include
the output data into the original table, you will have to use "mutate".
mutate(gps_roe_grouped, mean = mean(DOP))
Note that we didn't assign the output of this function to a variable, so the result
is just displayed in our console but not saved.

With this function, we have a powerful tool to get various information on very 
big datasets in a matter of seconds. But for now, we are limited to retrieving
information on the whole dataset per animal (e.g. the mean DOP per roe deer).
Many questions we will have are related to the relation between two sucessive 
gps-points, i.e. to sucessive rows, or a moving temporal window. dplyr can handle 
this easily, as we will see in the next script.

#############################################################################
#############################################################################
#############################################################################
#############################################################################
#############################################################################
#############################################################################

#############################################################################
#Calculate distance and speed using dplyr #################################
#############################################################################

In the previous script, we used dplyr to summarise data form a large dataset
into a very small one using "summarise()". We also used "mutate()" to add 
columns to our dataset. We were limited to retrieving information on the 
whole dataset per animal. Many questions we will have are related to the 
relation between sucessive gps-points, i.e. to sucessive rows. dplyr can 
handle this easily.

Let's say we want to calculate the sampling interval between sucessive fixes.
So we want dplyr to subtract DateTime value 2 from value 1, value 3 from 
value 2 and so fourth. Like this:

Value 2 - Value 1
Value 3 - Value 2
Value 4 - Value 3
...
minuend - subtrahend

From an R perspective, its much easier (and faster) to view this operation as
the substraction of two vectors of the same length (one vector being the minuend,   
one the subtrahend). The minuend vector LEADS by one value, which is handled in dplyr
as follows:

gps_roe_grouped <- mutate(gps_roe_grouped, timediff = lead(DateTime) - DateTime)
View(gps_roe_grouped)

Subtracting two POSIXct objects returns objects of the class "difftime":
str(gps_roe_grouped$timediff)

For our purposes, it will be easier to work with integers. So let's reformulate the
dplyr command:

gps_roe_grouped <- mutate(gps_roe_grouped, timediff = as.integer(lead(DateTime) - DateTime))
str(gps_roe_grouped$timediff)


We can now use the Eucledian distance function we customised to calculate 
the Eucledian distance between the rows:

gps_roe_grouped <- mutate(gps_roe_grouped, steplength = euclid(lead(X), lead(Y), X, Y))


let's again have a look at our steplengths using "hist"
hist(gps_roe_grouped$steplength)

Most steps are between 0 and 50 meters, and all steps are less than 1000 meters.
Now let's calculate the speed at which the roe deer travel between the gps-fixes.

Now let's calculate the speeds of all steps. If you think about it, 
we don't need dplyr for this step, since we're not calculating relations 
between rows, but between the columns "timediff" and "steplength". Let's use
dplyr anyway for the sake of practice. 

But before we calculate the speed, let's settle on a unit. Our timediff is in minutes 
and our steplength is in meters. Meters per minute doesn't make much sense since its 
not DIN conform. Meter per second is probabbly more useful for our case, so we will
have to multiply our timediff values by a factor 60. We have to keep in mind that our timediff
column is still in minutes, changing that to seconds as well would make the column
harder to read.

View(gps_roe_grouped)

gps_roe_grouped <- mutate(gps_roe_grouped, speed = steplength/(timediff*60))

let's inspect our data again using hist()
hist(gps_roe_grouped$speed)

most of the data lies between 0 and 0.1 m/s. But there is not much else we can 
take from this diagram. Let's take a closer look at the values 0 to 0.4 using xlim
hist(gps_roe_grouped$speed, xlim = c(0,0.4))

You can specify the size of the bin-size by using "breaks" and defining a sequence "seq()"
of numbers. Let's say we want a bin size of 0.01, we would go about this like this:
hist(gps_roe_grouped$speed, xlim = c(0,0.4), breaks = seq(0,1.5,0.01))

highlight and press ctrl+enter (windows) to see what the seq function does
seq(0,1.2,0.01)

It creates a sequence of number from 0 to 1.2 with a spacing of 0.01. If you recieve
an error message when defining the breaks, it is usually because your max value is
not within the sequance you defined. Raise the second value in the seq() to solve the
issue.

If you want to take a closer look at your "rare" values (e.g. speed of 1.2 m/s) you
can change the ylim rescaling the diagram. xlim and ylim can of course be combined.
hist(gps_roe_grouped$speed, breaks = seq(0,1.3,0.01), ylim = c(0,50))


#############################################################################
#############################################################################
#############################################################################
#############################################################################
#############################################################################
#############################################################################

#############################################################################
#Using facet grid with ggplot #############################################
#############################################################################


We've been using the base-R "hist" function to visualise our data very quickly for
verifying purposes. To make a nice diagram to publish somewhere, it's smarter to use
ggplot which we already got to know. For example, let's make a boxplot of the different
traveling speeds of our three animals.

Before we start, let's fix an issue with our data. The "join" operation
in the last lesson produced an error message because the factor levels of the two tables
didn't match. If we look at the structure of our TierID column, we will see that our data
was actually modified (as the error message told us then, but we chose to ignore)
str(gps_roe_grouped$TierID)

ggplot can't deal with "character" as a grouping variable, so let's turn the column back 
into a factor. 
gps_roe_grouped$TierID <- factor(gps_roe_grouped$TierID)

Now we can create a boxplot, similiar to how we created the map in the last lesson:
ggplot(gps_roe_grouped, aes(TierID, speed)) +
  geom_boxplot() 

Not much to see here. So let's see what happens if we view the different days of the
week. For this we will need to add a column "weekday" to our data using the lubridate
function wday()
gps_roe_grouped$weekday <- lubridate::wday(gps_roe_grouped$DateTime,label = T)


Making subgroups with the variable "weekday" is done by adding a layer named "faced_grid()"
ggplot(gps_roe_grouped, aes(TierID, speed)) +
  geom_boxplot() +
  facet_grid(.~weekday)

Now we can see a distinct rise in outliers on friday, and lower "speeds" on saturday. 
If you dont like the fact that the week starts on sunday, you don't change this 
in ggplot, but you change the factor levels:
levels(gps_roe_grouped$weekday)

You can see that the first value is "sun". You can change the order like this:
http://www.cookbook-r.com/Manipulating_data/Changing_the_order_of_levels_of_a_factor/
gps_roe_grouped$weekday <- factor(gps_roe_grouped$weekday, 
                                  c(levels(gps_roe_grouped$weekday)[2:7],levels(gps_roe_grouped$weekday)[1]))

We could have done this writing the weekdays out, but that might lead to spelling mistakes:
gps_roe_grouped$weekday <- factor(gps_roe_grouped$weekday, 
c("Mon", "Tues", "Wed", "Thurs" "Fri", "Sat", "Sun" ))

Note that the language in which your weekdays appear is dependent on settings in your 
operating system, and hence may be different on your computer

Plot your data again, and voila, the week starts on monday
ggplot(gps_roe_grouped, aes(TierID, speed)) +
  geom_boxplot() +
  facet_grid(.~weekday)



#############################################################################
#############################################################################
#############################################################################
#############################################################################
#############################################################################
#############################################################################


#############################################################################
#Calculate timelags using move ############################################
#############################################################################

A while ago, we've talked about the package "move" and we did a first
map plot using this package. But since then we've addressed all our movement 
related tasks (steplength, timediff, speed) using dplyr. This was a very
good excercise, since now you have tools to do all sorts of crazy things 
with your data. 

But now, let's take a look at our move package again and see what tools we have 
there. We created a "moveStack" object a while ago, let's have a look at it again:
gps_roe_move

Safi Chapter 3.2 shows us that calculating time differences between gps-fixes
is achieved using this command:
timeLag(gps_roe_move, units = "mins")

This command retrieves the timelags, but doesn't add it to our original data.
Let's save the data to a variable and have a look at it:
timeLags <- timeLag(gps_roe_move, units = "mins")

Lets look at the structure of the output:
str(timeLags)


We can see that it's a list containing three numerical vectors. Let's have a closer
look at this list-object

#############################################################################
#A quick look at "list"-objects ###########################################
#############################################################################

A list is an object type (a collection) that can contain basically ANYTHING, even 
dataframes or other lists. Read a quick introduction to lists 
(and other data types in R) here:
http://www.statmethods.net/input/datatypes.html

Accessing the timelags of one animal is done like this:
timeLags[[1]] by index (note the double brackets)
timeLags[["RE02"]] or by name

if you use double brackets, the structure of your output will correspond to the
structure of the data that was added to the list. Earlier I wrote "timeLags" is a list 
containing 3 numerical vectors. So using [[]] should output a numeric vector. 
let's checK:
str(timeLags[[1]])

if you add single brackets, the structure of your ouptut remains a list (which has
some disadvantages)
str(timeLags[1])

If you want, can access a specific value of a specific object in a list as follows:
timeLags[[1]][1]

Using the function "unlist()" turns a list into a "named numeric vector". A named
numeric vector is just like a normal numeric vector, but each value has a corresponding
name. With a numeric vector, we can create something like a histogram (what we
cannot do with a list)
str(unlist(timeLags))

Let's make a side-by-side comparison via a histogram of the data we created 
using the two methos:
hist(unlist(timeLags), main = "Histogram using MOVE-data")
dev.new() opens a new plot window
hist(gps_roe_grouped$timediff, main = "Histogram using DPLYR-data")
the histograms seem to be identical. Let's close the new plot widow again:
dev.off()

Calculating the timeLags between n values returns n-1 values. dplyr solved
this by using "NA" (not available) for the last value of every roe deer. 
tail(gps_roe_grouped$timediff) tail shows the overall last values of the data.frame

Let's compare the number of values between the two methods:
length(unlist(timeLags)) - length(gps_roe_grouped$timediff)

Using the package move, we have 3 values less than using dplyr. This means every
roe deer is missing one value (the last one). We need to keep this in mind when  
working with "move".


#############################################################################
#Calculate distances, speed and turning angles using "move" ##############
#############################################################################

In the same way we calculated the timelags, move allows us to calculate
distance and speed between the gps points:
distances <- distance(gps_roe_move)
speeds <- speed(gps_roe_move)


Move allows us to calculate these values with little effort. We were able to
calculate them with dplyr as well, giving us the advantage of full control. 
Move has some additional functions, that would be rather tricky to implement
just with dplyr. One example is the function "angle()", which returns the
turning angle (see Safi 3.3.3). Again, you need only one command:
angle <- angle(gps_roe_move)

hist(unlist(angle))

tail(angle)





#############################################################################
#############################################################################
#############################################################################
#############################################################################
#############################################################################
#############################################################################

#############################################################################
#Multiscale analysis ######################################################
#############################################################################

As Safi states in Chapter 3.3.2, traveling speed is highly susceptible to 
scaling effects, and we have to be aware of the scaling issues that irregular 
sampling could have on speed.

Our roe deer data was sampled at two different sampling intervals: 3 hours
and 5 minutes. In this excercise, we want to compare the speeds that we 
can derive from these sampling intervals.

First, let's have a look our sampling intervals again:
hist(gps_roe_grouped$timediff, breaks = seq(0,400,10)) all of'em 
hist(gps_roe_grouped$timediff, breaks = seq(0,400,1), xlim = c(0,10)) the small intervals, by minute
hist(gps_roe_grouped$timediff, breaks = seq(0,400,1), xlim = c(170,190)) the large intervals, by minute

Most values are around 5 and 180 minutes. But the time intervals vary. So let's make groups
by rounding all values to the nearest 5 minutes. 
gps_roe_grouped$sampInt <- round(gps_roe_grouped$timediff/5,0)*5

We can now create a boxplot using the rounded values as groups. I will use 
the function "filter" dplyr) to select only only the finite values of 
our sampInt column (last gps fixpoint of every roe deer does not have a time 
differece therefore no sampInt-value). I will use a logarithmic y-axis so 
that I can see all values.

Note that I need to use the function "factor()" on our sampInt column since 
ggplot cannot handle numeric data as a grouping interval for boxplots.

ggplot(filter(gps_roe_grouped, is.finite(sampInt)), 
       aes(factor(sampInt), speed)) +
  geom_boxplot() +
  scale_y_log10() +
  annotation_logticks(sides = "l") +
  labs(y = "speed (m/s)") 

Since we only have a substantial amount of data for 5 minute and 180 minute 
intervals, it would be reasonable to plot only the values with a sampInt 
value of 5/180.

We won't be needing the "sampInt" column for a while now, so let's get rid of
it:
gps_roe_grouped$sampInt <- NULL



#############################################################################
#############################################################################
#############################################################################
#############################################################################
#############################################################################
#############################################################################

#############################################################################
#Defining functions #######################################################
#############################################################################

base-R has a LOT of useful functions that we can use for our purposes. 
In addition, there are thousands of packages providing millions of
additional functions that we can choose from. But sometimes, we want to
create our own custom functions to fit our needs exactly. 

If you create your own function, it is advisable to do this in the very beginning
of the project. I would put it right after importing the packages (because your
function might depend on a package) and right before importing your rawdata 
(because you might need a custom function to import your data). Create a file
named "Custom_Function.R" now and place it using source("Custom_Function.R") in 
the described position of your Masterfile.

In my project, I was looking for a way to calculate a simple Euclidean
distance between two points. "dist()" would take care of this, but I don't
like the way I need to input my data into the dist() function. Hence, I created
my own function so I can input my data the way I want to.

Calculating the Euclidean distance is achieved using this formula:
https://bigsnarf.files.wordpress.com/2012/03/distance.jpg
To convert this into an R function, we need to 
1) define a function name
2) define input variables
3) specify what our function should do with these variables
4) define how the results are returned

"euclid" is our function name
euclid <- function(x1,y1,x2,y2){ "x1, y1..." are the input variables
  the following line tells our function what to do with the variables:
  return(sqrt((x1-x2)^2+(y1-y2)^2)) 
}and thats it!

Note: The order in which you define your input variables will be the order  
R assumes your data is passed to the function if you don't explicitly
specify the variables. If you have the positions (600,200) and (800,400)
and you want to calculate the distance in between them, you can input you data
like this:
euclid(600,200,800,400)

If you want to change the order, for example like this:
euclid(600, 800, 200, 400)

You will get a wrong value unless you define your input variables using the
same variable names you specified while defining the function:
euclid(x1 = 600, x2 = 800, y1 = 200,y2 = 400)

futher note that x1, x2, y1 and y2 are LOCAL variables to your function. That means
that only your function knows what x1 stands for, you cannot call the value of x1 
outside the function:
x1

You can use the varible x1 in your project (GLOBAL variable) without creating 
confusion (except maybe, in your own head)
x1 = 600; y1 = 200; x2 = 800; y2 = 400
euclid(x1,y1,x2,y2)
ok, this is quite confusing, but not for R!:
euclid(x2 = x1, x1 = x2, y2 = y1, y1 = y2) 

Now include this custom function in your script "Custom_Function.R" and source it in 
the place specified in the beginning of this script.


