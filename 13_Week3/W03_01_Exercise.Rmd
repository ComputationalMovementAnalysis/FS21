# Exercise 3

## Learning Outcomes
+ You are able to segment a trajectory, e.g. using the approach proposed in @laube2011
+ You are able to compute the similarity between given trajectories using the package `SimilarityMeasures.`
+ You acquire further useful data processing skills.

## Prerequisites
Readings Skills from "R for Data Science" [@wickham2017]:

+ RS3.1 Chap1 Data visualization with `ggplot2` (31, 3-35)
+ RS3.2 Chap5 Exploratory Data Analysis (28p, 81.109)

Readings Theory:

@both2018 A Comparative Analysis of Trajectory Similarity Measures: Recommendations for Selection and Use, excerpt from an unpublished manuscript, confidential.

## Preperation

Install the following libraries:

```{r}
#- header3 Preperation
#- chunkstart
```


```{r, echo = T, include = T, eval = F}
install.packages("SimilarityMeasures")

# The following packages are for optional tasks:
install.packages("plotly")

# You don't really need the following packages, 
# we just use them in our figures
install.packages("ggrepel")
```




Open your R Project from last week. Either run your own script from last week or the following lines to transform the data into the form we need for today's exercise.

```{r, echo = T, include = T, eval = T}
library(tidyverse)
library(sf)

# Import as dataframe
wildschwein_BE <- read_delim("00_Rawdata/wildschwein_BE.csv",",")

# Convert to sf-object
wildschwein_BE = st_as_sf(wildschwein_BE, coords = c("Long", "Lat"), crs = 4326,remove = FALSE)

# transform to CH1903 LV95
wildschwein_BE <- st_transform(wildschwein_BE, 2056)

# Add geometry as E/N integer Columns
wildschwein_BE <- st_coordinates(wildschwein_BE) %>%
  cbind(wildschwein_BE,.) %>%
  rename(E = X) %>%
  rename(N = Y)

# Compute timelag, steplength and speed
wildschwein_BE <- wildschwein_BE %>%
  group_by(TierID) %>%
  mutate(
    timelag = as.numeric(difftime(lead(DatetimeUTC),DatetimeUTC,units = "secs")),
    steplength = sqrt((E-lead(E))^2+(N-lead(N))^2),
    speed = steplength/timelag
  )

```

```{r}
#- chunkend
```


## Tasks and Inputs

### Input: Segmentation as in Laube and Purves (2011)

```{r}
#- header3 Input
#- chunkstart
```


You've read @laube2011 about segmenting trajectories. In the paper, the authors define "*static*" fixes as "*those whose average Euclidean distance to other fixes inside a temporal window v is less than some threshold d*", as illustrated in the following figure:

![](02_Images/laube_2011.jpg)

The above image from @laube2011 visualizes the following steps:

(a) Temporal representation of constant sample interval with associated temporal window v for three exemplary points; 
(b) Measurement of average distance in temporal window v to sample points in spatial representation;
(c) Removal of all points where average distance is less than a given threshold, i.e. removal of static points; and 
(d) Removal of subtrajectories with less than a threshold temporal length. 

We will implement this method on the following dummy data. Once you've grasped the idea on this simple data, you will implement it for the wild boar data in task 1.

Note: I use `tibble()` instead of `data.frame()`. The two functions are very similar, this is just a matter of preference. 


```{r, echo = T, include = T, eval = T}
set.seed(10)
n = 20
df <- tibble(X = cumsum(rnorm(n)), Y = cumsum(rnorm(n)))

ggplot(df, aes(X,Y)) +
  geom_path() + 
  geom_point() +
  coord_equal()
```




#### Segmenting

The first step is calculating the distances to temporally close samples within the temporal window *v*. Take the following sample data, assuming the sampling interval is 5 minutes. If we take a temporal window of 20 minutes, that would mean including 5 fixes. We need to calculate the following Euclidean distances (pos representing single location):

1. `pos[n-2`] to `pos[n]`
2. `pos[n-1]` to `pos[n]`
3. `pos[n]` to `pos[n+1]`
4. `pos[n]` to `pos[n+2]`

Just like last week, we use the formular for calculating the Euclidean distance in in combination with `lead()` and `lag()`. For example, to create the necessary offset of n-2, we use `lag(x, 2)`. For each offset, we create one individual column.


```{r, echo = T, include = T, eval = T}

df <- df %>%
  mutate(
    nMinus2 = sqrt((lag(X,2)-X)^2+(lag(Y,2)-Y)^2),   # distance to pos -10 minutes
    nMinus1 = sqrt((lag(X,1)-X)^2+(lag(Y,1)-Y)^2),   # distance to pos - 5 minutes
    nPlus1  = sqrt((X-lead(X,1))^2+(Y-lead(Y,1))^2), # distance to pos + 5 mintues
    nPlus2  = sqrt((X-lead(X,2))^2+(Y-lead(Y,2))^2)  # distance to pos +10 minutes
  )

```

Now we want to calculate the mean distance of `nMinus2`, `nMinus1`, `nPlus1`, `nPlus2` for each row. The below function calculates the overall mean of all columns, which is not what we want.


```{r, echo = T, include = T, eval = T}
df %>%
  mutate(
    stepMean = mean(c(nMinus2, nMinus1,nPlus1,nPlus2), na.rm = T)
  )
```


Since we want the mean value *per Row*, we have to explicitly specify this before `mutate()` with the function `rowwise()`. Note the new grouping variable `<by row>` when printing the dataframe to the console.

```{r, echo = T, include = T, eval = T}
df <- df %>%
  rowwise() %>%
  mutate(
    stepMean = mean(c(nMinus2, nMinus1,nPlus1,nPlus2))
  )

df
```

We can now determin if an animal is moving or not by specifying a threshold on `stepMean`

```{r, echo = T, include = T, eval = T}

df <- df %>% 
  mutate(
    moving = stepMean>1.5
    )

df

ggplot(df, aes(X,Y)) +
  geom_path() + 
  geom_point(aes(colour = moving)) +
  coord_equal()
```



#### Unique IDs *per segment*

When segmenting trajectories, we often want to compute metrics on the basis of *each segment*. Within the tidyverse logic, we need a unique ID per segment that we can pass to `group_by()`. In other words, we need a unique ID for a sequence of successive `TRUE` values. For lack of a better way, we suggest solving this problem with `cumsum()`.

`cumsum()` returns the cummulative sum of a given vector:
```{r, echo = T, include = T, eval = T}
one_to_ten <- 1:10
one_to_ten
cumsum(one_to_ten)
```

In R, `TRUE`and `FALSE` are interprated as `1` and `0` if coerced to an integer. 
```{r, echo = T, include = T, eval = T}
as.integer(TRUE)
as.integer(FALSE)

TRUE+TRUE
```

Therefore, `cumsum()` on a boolean vector increases the count on each `TRUE` value:

```{r, echo = T, include = T, eval = T}

boolvec <- c(FALSE,TRUE,TRUE,TRUE,FALSE,FALSE,TRUE,TRUE)

df_cumsum <- tibble(boolvec = boolvec,cumsum = cumsum(boolvec))

df_cumsum
```

You might have noticed that this is pretty much exactly the opposite of what we need. We therefore have to take the inverse of the boolean vector:

```{r, echo = T, include = T, eval = T}

df_cumsum %>%
  mutate(
    boolvec_inverse = !boolvec,
    cumsum2 = cumsum(boolvec_inverse)
  )


```


To simplify our workflow, we can we can take the inverse of `boolvec` *within* our `cumsum()` statement and save an extra line of code.


```{r, echo = T, include = T, eval = T}
df_cumsum %>%
  mutate(
    cumsum2 = cumsum(!boolvec)
  )
  
```

```{r}
#- chunkend
```

### Task 1: Segmentation

```{r}
#- header3 Task1
#- chunkstart
```

With the skills from the input above we can now implement the segmentation algorithm described in @laube2011. The described method depends on a regular sampling interval. Therefore, take the dataset "caro60.csv" from task 3 of last week (available on moodle). Import it as a dataframe, we don't need an `sf`-object to for today's tasks.

Next, we have to have to define our *temporal window v* [@laube2011]. To keep things simple, I would suggest a window of n +/- 2 minutes. With a sampling interval of around 1 minute, this corresponds to a window size of n +/- 2 positions.

```{r}


caro60 <- read_delim("00_Rawdata/caro60.csv",",")

caro60 <- caro60 %>%
  mutate(
    stepMean = rowMeans(                       # We present here a slightly different 
      cbind(                                   # approach as presented in the input
        sqrt((lag(E,2)-E)^2+(lag(E,2)-E)^2),   # cbind() creates a matrix with the same 
        sqrt((lag(E,1)-E)^2+(lag(E,1)-E)^2),   # number of rows as the original dataframe,
        sqrt((E-lead(E,1))^2+(E-lead(E,1))^2), # but with four columns, and rowMeans() returns 
        sqrt((E-lead(E,2))^2+(E-lead(E,2))^2)  # a single vector (again, with the same number 
        )                                      # of rows as the original dataframe)
      )
  )
```


```{r}
#- chunkend
```

### Task 2: Specify and apply threshold *d*

```{r}
#- header3 Task 2
#- chunkstart
```

After calculating the Euclidean distances to positions within the temporal window *v* in task 1, you can explore these values (we stored them in the column `stepMean`) using summary statistics (histograms, boxplot, `summary()`): This way we can define a reasonable threshold value to differentiate between "stops" and "moves". There is no "correct" way of doing this, specifying a threshold always depends on data as well as the question that needs to be answered. In this exercise, find a threshold that matches your intuition. 

Store the new information (boolean to differentiate between stops (`TRUE`) and moves (`FALSE`)) in a new column named `moving`.



```{r, echo = F, include = T, eval = T, fig.cap="My summary statistics shows a mean of 5, the histogram shows that this might be a reasonable threshold."}

summary(caro60$stepMean)

ggplot(caro60, aes(stepMean)) +
  geom_histogram(binwidth = 1) +
  geom_vline(xintercept = 5)
```

```{r}
caro60 <- caro60 %>%
  mutate(
    moving = stepMean > 5
  ) 

```




```{r}
#- chunkend
```

### Task 3: Visualize segmented trajectories
```{r}
#- header3 Task 3
#- chunkstart
```

Now visualize the segmented trajectory spatially. Just like last week, you can use ggplot with `geom_path()`, `geom_point()` and `coord_equal()`. Assign `colour = moving` within `aes()` to distinguish between segments *with* "movement" and *without*.

 

```{r, echo = F, include = T, eval = T}

p1 <- caro60 %>%
  ggplot() +
  geom_path(aes(E,N), alpha = 0.5) +
  geom_point(aes(E,N,colour = moving)) +
  theme_minimal() +
  coord_equal()

p1

```

Tip: if you want to get a `ggplot()` with Zoom capability, just load the library `plotly` and wrap the `ggplot()` function with `ggplotly()`


```{r, echo = F, include = T, eval = knitr::is_html_output(), fig.cap="This is the same plot as above, wrapped in the function `ggplotly()` from the library `plotly`. With this function, the plot becomes interactive. Try it out!"}

library(plotly)
ggplotly(p1)

```

```{r, echo = F, include=T, eval = knitr::is_latex_output(), results = "asis", purl = F}
pander::pandoc.image("02_Images/batman-ipsum.png","The interactive map/plot is only available in the online version of this document.")
```


```{r}
#- chunkend
```
### Task 4: Segment-based analysis

```{r}
#- header3 Task 4
#- chunkstart
```

In applying @laube2011, we've come as far as steps (b)/(c) in [Figure 1](02_Images/laube_2011.jpg). In order to complete step (d) (*Removal of subtrajectories with less than a threshold temporal length*), we have to calculate each segment's temporal duration. 

In order to do this, we need a *unique* name for each segment that we can use as a grouping variable. This is where the `cumsum()` approach which we introduced in the input comes useful. Complete the following steps:

1. Filter the data by removing all rows where `moving` equals to `NA` (typically these are the the first and last 2 rows). 
2. Get a unique ID per segment and using the `cumsum()` approach we introduced in the input. Store this ID in a new column `segment_ID`
3. Filter the data to remove all rows where the animal is not moving 
4. Group by the column `segment_ID` to calculate the temporal length of each segment with `mutate()`.
5. Remove segments with a duration less than 3 minutes
6. inspect your data visually

```{r, echo = F, include = T, eval = T}

caro60_moveseg <-caro60 %>%
  filter(!is.na(moving)) %>%
  ungroup() %>%
  mutate(
    segment_ID = as.factor(1+cumsum(!moving))
  )  %>%
  filter(moving) %>%
  group_by(segment_ID) %>%
  mutate(
    segment_duration = as.integer(difftime(max(DatetimeUTC),min(DatetimeUTC),units = "mins"))
  ) %>%
  filter(segment_duration >= 3)



bind_rows(mutate(caro60, lab = "before"),mutate(caro60_moveseg,lab = "after")) %>%
  mutate(lab = fct_rev(lab)) %>%
  ggplot(aes(E,N, colour = segment_ID)) +
  geom_path(alpha = 0.5) +
  geom_point() +
  theme_minimal() +
  coord_equal() + 
  facet_wrap(~lab)



```

```{r}
#- chunkend
```

### Task 5: Similarity measures 

```{r}
#- header3 Task 5
#- chunkstart
```


Import the dataset `pedestrian.csv` (available on moodle) as a dataframe (you don't need an `sf` object). It it a set of six different but similar trajectories from pedestrians walking on a path. Explore this data visually.

We will analyse these trajectories with the package `SimilarityMeasures`, always comparing trajectory 1 pairwise to the other trajectories 2-6. For this task, explore the trajectories first and get an idea on how the pedestrians moved. We step away from using the wild boar data for this task because our animals don't express the type of similarity we want to illustrate here. Also, using the constructed pedestrian data allows us illustrating very typical similarity issues, that are picked-up in different ways by the different similarity measures. In later exercises we will get back to our wild boar!


```{r}

library(ggrepel)
pedestrians <- read_delim("00_Rawdata/pedestrian.csv",",")

pedestrians <- pedestrians %>%
  group_by(TrajID) %>%
  mutate(index = row_number()) %>%
  ungroup() %>%
  mutate(TrajID = as.factor(TrajID))


plotraj <- function(idx,lab = F){
  dat <- pedestrians %>%
    filter(TrajID %in% c(1,idx))
  
  p <- ggplot(dat, aes(E,N, colour = TrajID, label = index)) +
    geom_path(colour = "grey", alpha = 0.5) + 
    geom_point() + 
    # scale_color_discrete(guide = "none") +
    # labs(title = paste("Trajectories 1 and",idx)) +
    theme_minimal() +
    coord_equal()
  
  if(lab == T){
    p <- p + 
      geom_text_repel(data = filter(dat,index == 1 | index %% 2 == 0),
                      aes(E,N,label = index,colour = TrajID),
                      show.legend = FALSE) 
  }
  
  p
}

```


```{r, echo = F, include = T, eval = T, fig.cap="Comparing Trajectories 1 and 2"}
plotraj(2)


```

```{r, echo = F, include = T, eval = T, fig.cap="Comparing Trajectories 1 and 3. Some positions are labelled with the position index to illustrate the direction of movement per trajectory."}
plotraj(3,T) 


```

```{r, echo = F, include = T, eval = T, fig.cap="Comparing Trajectories 1 and 4"}
plotraj(4)

```

```{r, echo = F, include = T, eval = T, fig.cap="Comparing Trajectories 1 and 5"}
plotraj(5)

```

```{r, echo = F, include = T, eval = T, fig.cap="Comparing Trajectories 1 and 6"}

plotraj(6)
```

```{r}
#- chunkend
```

### Task 6: Calculate similarity

```{r}
#- header3 Task 6
#- chunkstart
```

Calculate the similarity between trajectory 1 and trajectories 2-6. Use the different similarity measures in the package `SimilarityMeasures`. Visualize your results and try to understand the different results with respect to your reading of @both2018. Can you see connections between the properties of the trajectories and the similarity values computed by the different measures?

Note: 

- All functions in the package need matrices as input, with one trajectory per matrix. 
- `LCSS`takes very long to compute. The accuracy of the algorithm (`pointSpacing =` ,`pointDistance =` and `errorMarg =`) can be varied to provide faster calculations. Please see @vlachos2002 for more information.



```{r}

# instead of repeating the same step 6 times, I use purrr::map() 
# which creates a list of dataframes. Feel free to use a method
# with which you feel comfortable.

pedestrians_matrix <- pedestrians %>%
  split(.$TrajID) %>%
  map(function(x){
    x %>%
      select(E,N) %>%
      as.matrix()
  })

```


```{r, echo = F, include=T, eval = T, fig.cap="Comparing Trajectory 1 to trajectories 2 to 6"}
library(SimilarityMeasures)

# Again, we use one of the purrr::map_* family of functions
# to calculate three indicies over all 5 pairs in one go.
# As before: feel free to use a different method you feel 
# more comfortable in.


pedest_measures <- imap_dfr(pedestrians_matrix, ~data_frame(
  traj = .y,
  DTW = DTW(.x,pedestrians_matrix[[1]]),
  EditDist = EditDist(.x,pedestrians_matrix[[1]]),
  Frechet = Frechet(.x,pedestrians_matrix[[1]]),
  LCSS = LCSS(.x,pedestrians_matrix[[1]],5,4,4)
  ))



pedest_measures %>%
  gather(key,val,-traj) %>%
  ggplot(aes(traj,val, fill = traj))+ 
  geom_bar(stat = "identity") +
  facet_wrap(~key,scales = "free") +
  theme(legend.position = "none") +
  labs(x = "Comparison trajectory", y = "Value", title = "Computed similarities using different measures \nbetween trajectory 1 to all other trajectories ")

```



```{r code=readLines('01_R_Files/clear_workspace.R'), echo = F, include=F, eval=T}
```





