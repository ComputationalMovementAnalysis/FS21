[["index.html", "Patterns and Trends in Environmental Data Master ENR, Spring Semester 2021 Introduction Chapter", " Patterns and Trends in Environmental Data Master ENR, Spring Semester 2021 Patrick Laube, Nils Ratnaweera, Nikolaos Bakogiannis 22 April, 2021 Introduction Chapter For our practical R course building-up skills for analyzing movement data in the software environment R, youll be using data from the ZHAW project Prävention von Wildschweinschäden in der Landwirtschaft. Watch this video on youtube for a nice introduction into the project. The project investigates the spatiotemporal movement patterns of wild boar (Sus scrofa) in agricultural landscapes. We will study the trajectories of these wild boar, practicing the most basic analysis tasks of Computational Movement Analysis (CMA). Please note: we are given application data from an ongoing research project. Capturing wild living animals and then equipping them with GPS collars is a very labor and cost intensive form of research. Consequently, data resulting such campaigns is a very valuable asset that must be protected. So, please do not pass on this data, for any use beyond this module contact Patrick Laube or the data owner Stefan Suter (suts@zhaw.ch). "],["W01_01_exercise.html", "Exercise 1", " Exercise 1 This exercise covers the necessary steps for getting ready in R and some basic concepts for setting up a well-structured R project. The lesson introduces how additional packages that provide useful functions for data science are made available and how spatial data is handled. The exercise concludes with the creation of your first map featuring movement data. "],["W01_02_outcomes.html", "Learning outcomes", " Learning outcomes You learn how to structure an R project. You can read movement data from a .csv-file into a data.frame You can convert spatial point data from a data.frame to a spatial object sf You can perform basic spatial operations on spatial objects in R You can produce simple maps of your spatial data using ggplot2 You can produce simple maps of your spatial data using tmap "],["W01_03_prerequesites.html", "Prerequisites", " Prerequisites Readings Skills from R for Data Science (Wickham and Grolemund 2017): RS1.1 Preface (16p, ix-xxiv) RS1.2 Chap2 Workflow basics (3p, 37-39) RS1.3 Chap4 Workflow scripts (3p, 77-79) RS1.4 Chap6 workflow projects (6p, 111-116) RS1.5 Chap8 Data Import with readr (21p) RS1.6 Chap13 Date and Times with lubridate (18p, 237-256) "],["W01_04_preperations.html", "Preperation", " Preperation Check you version of R Check your Version of R by opening RStudio and typing the following command into the console. R.version.string ## [1] &quot;R version 4.0.3 (2020-10-10)&quot; This returns the version number of your R installation, whereas the first digit (4) indicates the number of the major release, the second digit (0) indicates the minor release and the last digit (3) refers to the patch release. As a general rule of thumb, you will want to update R if you dont have the current major version or are lagging two (or more) versions behind the current minor release In the time of writing (April, 2021), the current R Version is 4.0.5 (released on 2021-03-31 07:05:15, see cran.r-project.org). Your installation should therefore not be older than 4.0.0. If it is, make sure that you have updated R until next week (doing it now will probably take too long). Check these instructions on how to update R Check your version of RStudio RStudio is the Graphical User Interface (GUI) we use in our course to interact with R. RStudio should not be too old either and we recommend updating if you dont have the latest version: check if this is the case by clicking on help &gt; check for updates. If you need to update RStudio, dont update now but have a newer version of RStudio installed before next week. Install the necessary packages If you havent already, install the packages tidyverse, sf and terra(using install.packages()). install.packages(&quot;tidyverse&quot;) install.packages(&quot;sf&quot;) install.packages(&quot;raster&quot;) Install Git Next, install Git. If you are not sure whether you already have Git installed or not, you can verify this by typing git --version in the terminal (there is a terminal built into RStudio which you can use for this. By default, it is situated in the bottom left corner in the tab named Terminal). If this command returns a version number you have Git installed already and might only need to update it. If this command returns git: command not found (or something similar), you will need to install Git first. Windows: We recommend installing Git for Windows, also known as msysgit or Git Bash. When asked about Adjusting your PATH environment, select Git from the command line and also from 3rd-party software RStudio prefers Git to be installed in C:/Program Files/Git, we recommend following this convention Otherwise, we believe it is good to accept the defaults macOS: We recommend you install the Xcode command line tools (not all of Xcode), which includes Git Go to the shell and enter xcode-select --install to install developer command line tools Linux: On Ubuntu or Debian Linux: sudo apt-get install git On Fedora or RedHat Linux: sudo yum install git Configure RStudio Now we will set some RStudio Global options. But first, close all instances of RStudio and restart it (!!!). Then go to Tools &gt; Global options. R General Deactivate the option Restore .RData into workspace at startup Set Save workspace to .RData on exit to Never Git / SVN Activate the option Enable version control interface for RStudio projects If the Field Git executable: shows (Not Found), browse to your git installation (previous step). This path should look something like this: Windows: C:/Program Files/Git/bin/git.exe (not C:/Program Files/Git/cmd/git.exe or git-bash.exe) Linux / macOS: /usr/bin/git Introduce yourself to Git Now it is time to introduce yourself to git. In the terminal, enter your name and email address by entering the following commands: git config --global user.name &quot;Maria Nusslinger&quot; git config --global user.email &quot;nussmar@email.com&quot; Of course, replace the name and address with your credentials. Use the email address that you will use to create your Github account (which we will do next week). Prepare the folder structure for this course By this point, you probably have created a folder for this course somewhere on your computer. In our example, we assume this folder is located here: C:/Users/yourname/semester2/Modul_CMA (mentally replace this with your actual path). Before we dive into the exercises, take a minute to think about how you are going to structure your files in this folder. This course will take place over 7 weeks, and in each week you will receive or produce various files. We recommend creating a separate folder for each week, and one folder for the semester project, like so: Course Folder (C:\\\\Users\\\\yourname\\\\semester2\\\\Modul_CMA) ¦--week1 ¦--week2 ¦--week3 ¦--week4 ¦--week5 ¦--week6 ¦--week7 °--semester_project For the R-exercises that take place in weeks 1 to 5, we recommend that you create a new RStudio Project each week in subdirectory of the appropriate week. For example, this week your folder structure could look like this: Folder Week 1 (C:\\\\Users\\\\yourname\\\\semester2\\\\Modul_CMA\\\\week1) ¦--slides.pdf ¦--my_notes.docx ¦--seminar_screenshot.jpg °--week1-rexercise ¦--week1-r.Rproj ¦--wildschwein.csv °--my_solution.Rmd Note: the RStudio Project is located in a subfolder of C:/Users/yourname/semester2/Modul_CMA/week1 and named week1-rexercise. week1-rexercise is the projects directory name and the project name we realize that that the week number is redundant, there is a reason1 for this this means each week is a fresh start (which has pros and cons) Create an RStudio project for the first week Create a new RStudio Project (File &gt; New Project &gt; New Directory &gt; New Project). Click on Browse and switch to your equivalent of the folder C:/Users/yourname/semester2/Modul_CMA/week1 (the project we are about to initiate will be be created in a subdirectory of this folder). Click on open to confirm the selection In the field Directory name, type week1-rexercise. This will be the name of your RStudio project and the its parent directory. Check the option Create a git repository Click on Create Project You will see the project names of all your RStudio Projects listed in RStudio. Having the week number in the project name keeps you from getting confused on which project you are working on. "],["W01_05_tasks_and_inputs.html", "Tasks and inputs", " Tasks and inputs Before starting with the task: make sure you have read and followed the instructions in section Preperation In RStudio, open the RStudio Project you created for this week if you havent done so already. You can see that you are in an RStudio Project if the projects name is visible next to the little RStudio logo in the top right corner of RStudio (otherwise it will read Project: (None)). Download the wildboar movement data here: wildschwein_BE.csv (right click Save target as..) lastly, create a new R- (or RMarkdown) file and begin by loading the following packages: library(readr) # to import tabular data (e.g. csv) library(dplyr) # to manipulate (tabular) data library(ggplot2) # to visualize data Once you have set everything up, commit your file to your git repo in the following manner: Save your R/RMarkdown file Switch to the Git-Tab in the pane in the top right corner Click commit to open the commit-Window Click in the checkbox left of the file you want to commit Add a commit message to explain what you are commiting (e.g. initial commit) Click on commit to commit your changes Task 1: Import data In section data import, import the file wildschwein_BE.csv. Obtain this file from moodle. Assign correct data types as necessary and make sure the time zone is set correctly for the date/time column. Note: We recommend using the readr package to import your data (they all begin with read_*, note the underscore). These functions are less error prone than the base R functions (read.*, note the period). Specifically for the wild boar data, we recommend read_delim(). For everyone working on the RStudio Server: You will first need to upload this data to the server using the upload-button in the Files tab. Commit your changes as described in the beginning. Write a meaningful commit message (e.g. completed task 1). Task 2: Explore Data We will use a range of different visualization tools (i.e. R-packages) in this course. Several packages techniques have emerged in recent years, each with their specific strengths and weaknesses. While base::plot()is quick and simple, it not very scalable with growing complexity. ggplot2 offers solutions for most use cases and has an elegant, consistent syntax that is easy to get accustomed to. We will get to know other techniques later in the course. Get an overview of your data by creating a first map-like plot of your data producing a simple scatter plot with ggplot2. Setting up a ggplot with our data is done using the command ggplot(wildschwein_BE, aes(Long, Lat, colour = TierID)). Creating a map is done via the basic scatter plot command geom_point(). Assigning every individual its own colour is done using the ggplot argument colour =. Save your code in the appropriate section. Figure 1: You plot should look something like this. Commit your changes as described in the beginning. Input: Handling spatial data Until now, weve stored our location data within data frames as Lat/Long columns. This works well for many tasks, but sometimes we need special spatial classes to handle our trajectories. We will get to know such cases in our next tasks, but first we need to convert our data.frame into a spatial object. We will largely rely on sfwhen working with vector data in R. In order to transform our data.frame into an sf object, we need to use the function st_as_sf() while specifying the columns storing the coordinates and the coordinate reference system. (At this point, we assume you know what a Coordinate Reference Systems is. Check out this link if this is not the case.) library(sf) wildschwein_BE_sf &lt;- st_as_sf(wildschwein_BE, coords = c(&quot;Long&quot;, &quot;Lat&quot;), crs = 4326) Notice how st_as_sf takes the EPSG code for the crs = argument. You can find a lot of useful information on Coordinate Reference Systems (including EPSG Codes, etc.) under epsg.io. Lets compare our original data.frame with this new sf object: wildschwein_BE ## # A tibble: 51,246 x 6 ## TierID TierName CollarID DatetimeUTC Lat Long ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 002A Sabi 12275 2014-08-22 21:00:12 47.0 7.05 ## 2 002A Sabi 12275 2014-08-22 21:15:16 47.0 7.05 ## 3 002A Sabi 12275 2014-08-22 21:30:43 47.0 7.05 ## 4 002A Sabi 12275 2014-08-22 21:46:07 47.0 7.05 ## 5 002A Sabi 12275 2014-08-22 22:00:22 47.0 7.05 ## 6 002A Sabi 12275 2014-08-22 22:15:10 47.0 7.05 ## 7 002A Sabi 12275 2014-08-22 22:30:13 47.0 7.05 ## 8 002A Sabi 12275 2014-08-22 22:45:11 47.0 7.05 ## 9 002A Sabi 12275 2014-08-22 23:00:27 47.0 7.05 ## 10 002A Sabi 12275 2014-08-22 23:15:41 47.0 7.05 ## # ... with 51,236 more rows wildschwein_BE_sf ## Simple feature collection with 51246 features and 4 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: 7.019889 ymin: 46.97125 xmax: 7.112075 ymax: 47.01882 ## geographic CRS: WGS 84 ## # A tibble: 51,246 x 5 ## TierID TierName CollarID DatetimeUTC geometry ## * &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;POINT [°]&gt; ## 1 002A Sabi 12275 2014-08-22 21:00:12 (7.049618 46.99317) ## 2 002A Sabi 12275 2014-08-22 21:15:16 (7.049509 46.99416) ## 3 002A Sabi 12275 2014-08-22 21:30:43 (7.049406 46.99383) ## 4 002A Sabi 12275 2014-08-22 21:46:07 (7.049217 46.99375) ## 5 002A Sabi 12275 2014-08-22 22:00:22 (7.049359 46.99375) ## 6 002A Sabi 12275 2014-08-22 22:15:10 (7.049363 46.99382) ## 7 002A Sabi 12275 2014-08-22 22:30:13 (7.049326 46.99387) ## 8 002A Sabi 12275 2014-08-22 22:45:11 (7.049237 46.99395) ## 9 002A Sabi 12275 2014-08-22 23:00:27 (7.048383 46.99481) ## 10 002A Sabi 12275 2014-08-22 23:15:41 (7.049396 46.99373) ## # ... with 51,236 more rows As you can see, st_as_sf() has added some metadata to our dataframe (geometry type, dimension, bbox, epsg and proj4string) and replaced the columns Lat and Long with a column named geometry. Other than that, the new sf object is very similar to our original dataframe. In fact, sf objects are essentially dataframes, as you can verify with the function is.data.frame(): is.data.frame(wildschwein_BE_sf) ## [1] TRUE All operations we know from handling data.frames can be used on the sf object. Try some out! # subset rows wildschwein_BE_sf[1:10,] wildschwein_BE_sf[wildschwein_BE_sf$TierName == &quot;Sabi&quot;,] # subset colums wildschwein_BE_sf[,2:3] Instead of keeping the same data twice (once as a data.frame, and once as an sf object), we will overwrite the data.frame and continue working with the sf object from now on. This saves some memory space in R and avoids confusion. wildschwein_BE &lt;- st_as_sf(wildschwein_BE, coords = c(&quot;Long&quot;, &quot;Lat&quot;), crs = 4326) rm(wildschwein_BE_sf) # we can remove this sf object, since it just eats up our memory Task 3: Project data from WGS84 So what can we do with our new sf object that we couldnt before? One example is projecting the WGS84 (Lat/Long) coordinates into the new Swiss CRS CH1903+ LV952. Do this by using the function st_transform. By the way, do you notice a pattern here? The package sf names most functions for spatial operations with the prefix st_*, just as in PostGIS. Heres the resulting sf object from the operation: wildschwein_BE ## Simple feature collection with 51246 features and 4 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: 2568153 ymin: 1202306 xmax: 2575154 ymax: 1207609 ## projected CRS: CH1903+ / LV95 ## # A tibble: 51,246 x 5 ## TierID TierName CollarID DatetimeUTC geometry ## * &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;POINT [m]&gt; ## 1 002A Sabi 12275 2014-08-22 21:00:12 (2570409 1204752) ## 2 002A Sabi 12275 2014-08-22 21:15:16 (2570402 1204863) ## 3 002A Sabi 12275 2014-08-22 21:30:43 (2570394 1204826) ## 4 002A Sabi 12275 2014-08-22 21:46:07 (2570379 1204817) ## 5 002A Sabi 12275 2014-08-22 22:00:22 (2570390 1204818) ## 6 002A Sabi 12275 2014-08-22 22:15:10 (2570390 1204825) ## 7 002A Sabi 12275 2014-08-22 22:30:13 (2570387 1204831) ## 8 002A Sabi 12275 2014-08-22 22:45:11 (2570381 1204840) ## 9 002A Sabi 12275 2014-08-22 23:00:27 (2570316 1204935) ## 10 002A Sabi 12275 2014-08-22 23:15:41 (2570393 1204815) ## # ... with 51,236 more rows Commit your changes as described in the beginning. Input: Calculate Convex Hull Transforming from one Coordinate Reference System to another was one operation where we needed an object with a spatial nature. In this way, we were able to use an off the shelf function to project the coordinates from one CRS to another. In our next example, we again rely on a spatial function: We want to calculate a convex hull per Wild boar. And guess what the function for calculating a convex hull is called in sf? If you guessed st_convex_hull(), you were right! By default st_convex_hull() calculates the convex hull per feature, i.e. per point in our dataset. This of course makes little sense. In order to calculate the convex hull per animal, we need to convert our point- to multipoint-features where each feature contains all positions of one animal. This is achieved in two steps: First: add a grouping variable to the sf object. Note the new grouping variable in the metadata of the sf object. Other than that, group_by has no effect on our sf object. wildschwein_BE_grouped &lt;- group_by(wildschwein_BE,TierID) wildschwein_BE_grouped ## Simple feature collection with 51246 features and 4 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: 2568153 ymin: 1202306 xmax: 2575154 ymax: 1207609 ## projected CRS: CH1903+ / LV95 ## # A tibble: 51,246 x 5 ## # Groups: TierID [3] ## TierID TierName CollarID DatetimeUTC geometry ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;POINT [m]&gt; ## 1 002A Sabi 12275 2014-08-22 21:00:12 (2570409 1204752) ## 2 002A Sabi 12275 2014-08-22 21:15:16 (2570402 1204863) ## 3 002A Sabi 12275 2014-08-22 21:30:43 (2570394 1204826) ## 4 002A Sabi 12275 2014-08-22 21:46:07 (2570379 1204817) ## 5 002A Sabi 12275 2014-08-22 22:00:22 (2570390 1204818) ## 6 002A Sabi 12275 2014-08-22 22:15:10 (2570390 1204825) ## 7 002A Sabi 12275 2014-08-22 22:30:13 (2570387 1204831) ## 8 002A Sabi 12275 2014-08-22 22:45:11 (2570381 1204840) ## 9 002A Sabi 12275 2014-08-22 23:00:27 (2570316 1204935) ## 10 002A Sabi 12275 2014-08-22 23:15:41 (2570393 1204815) ## # ... with 51,236 more rows Second: use summarise() to dissolve all points into a mulipoint object. wildschwein_BE_smry &lt;- summarise(wildschwein_BE_grouped) wildschwein_BE_smry ## Simple feature collection with 3 features and 1 field ## geometry type: MULTIPOINT ## dimension: XY ## bbox: xmin: 2568153 ymin: 1202306 xmax: 2575154 ymax: 1207609 ## projected CRS: CH1903+ / LV95 ## # A tibble: 3 x 2 ## TierID geometry ## &lt;chr&gt; &lt;MULTIPOINT [m]&gt; ## 1 002A ((2568903 1206200), (2568925 1206207), (2568980 1206197), (2569024 120~ ## 2 016A ((2569231 1205823), (2569245 1205925), (2569247 1206027), (2569251 120~ ## 3 018A ((2568153 1205611), (2568155 1205613), (2568161 1205624), (2568162 120~ Now we can run st_convex_hull on the new sf object. mcp &lt;- st_convex_hull(wildschwein_BE_smry) Task 4: Ploting spatial objects Using base plot to visualize sf objects is easy enough, just try the following code. plot(mcp) But since we use ggplot extensively, try and plot the object mcp with ggplot. Hint: Use the layer geom_sf() to add an sf object. Note: ggplot refuses to use our specified CRS, so we need to force this by specifying datum = in coord_sf(). Try it out. Commit your changes as described in the beginning. Input: Importing raster data In the next task, we would like to add a background map to our mcp object. To do this, we have to the raster data into R first. For this, we use the package terra with the function rast. library(terra) ## terra version 1.0.10 ## ## Attaching package: &#39;terra&#39; ## The following objects are masked from &#39;package:dplyr&#39;: ## ## collapse, desc, near ## The following object is masked from &#39;package:glue&#39;: ## ## collapse pk100_BE &lt;- terra::rast(&quot;00_Rawdata/pk100_BE.tif&quot;) pk100_BE ## class : SpatRaster ## dimensions : 1821, 2321, 3 (nrow, ncol, nlyr) ## resolution : 5, 5 (x, y) ## extent : 2567000, 2578605, 1199996, 1209101 (xmin, xmax, ymin, ymax) ## coord. ref. : +proj=somerc +lat_0=46.9524055555556 +lon_0=7.43958333333333 +k_0=1 +x_0=2600000 +y_0=1200000 +ellps=bessel +towgs84=674.374,15.056,405.346,0,0,0,0 +units=m +no_defs ## source : pk100_BE.tif ## names : pk1_1, pk1_2, pk1_3 ## min values : 0, 0, 0 ## max values : 255, 255, 255 pk100_BE_2056.tif is a three layered geotiff File. The above console output shows some metadata including the resolution, extent and the names of our layers (pk1_1, pk1_2etc). With the default plot method, each layer is displayed individually: plot(pk100_BE) With plotRGB all three layers are combined into a single image: plotRGB(pk100_BE) Task 5: Adding a background map There are multiple ways to add a background map in ggplot, many require additional packages. This is a good opportunity to get to know a completely different package for creating maps: tmap (thematic map). This package was developed with a syntax very similar to ggplot2, which makes it easy to learn. library(tmap) tm_shape(pk100_BE) + tm_rgb() ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;, prefer_proj ## = prefer_proj): Discarded datum Unknown based on Bessel 1841 ellipsoid in Proj4 ## definition ## stars object downsampled to 1129 by 886 cells. See tm_shape manual (argument raster.downsample) As you can see, plotting layers in tmap is combined with the + sign, just as in ggplot2. In tmap however, each layer consists of two objects: a tm_shape() in which the data is called, and a tm_* object in which we define how the data is visualized (tm_rgb() states that it is plotted as an RGB Raster Layer). Add the object mcp to the plot in this manner. Read the vignette if you are having trouble. ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;, prefer_proj ## = prefer_proj): Discarded datum Unknown based on Bessel 1841 ellipsoid in Proj4 ## definition ## stars object downsampled to 1129 by 886 cells. See tm_shape manual (argument raster.downsample) ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;, prefer_proj ## = prefer_proj): Discarded datum Unknown based on Bessel 1841 ellipsoid in Proj4 ## definition ## stars object downsampled to 1129 by 886 cells. See tm_shape manual (argument raster.downsample) Commit your changes as described in the beginning. Task 6: Create an interactive map Rerun the tmap()... command from the previous task, but switch the plotting mode to view\" (tmap_mode(\"view\")) beforehand. Omit the raster layer (pk100_BE), you wont be needing it. Commit your changes as described in the beginning. As weve mentioned in the first Input, you can look up the EPSG codes under (epsg.io)[http://epsg.io]. For information specific to Switzerland, check the swisstopo website "],["W02_01_exercise.html", "Exercise 2", " Exercise 2 "],["W02_01_learning_outcomes.html", "Learning Outcomes", " Learning Outcomes You understand the dplyr functions mutate, summarise and group_by and can apply them to sf objects You can derive movement parameters (timelag, steplength, speed) from trajectory data. You can re-sample your trajectory data for cross-scale movement analysis. "],["W02_02_prerequisites.html", "Prerequisites", " Prerequisites Readings Skills from R for Data Science (Wickham and Grolemund 2017): RS2.1 Chap3 Data Transformation with dplyr (31p, 43-76) RS2.2 Chap10 Relational data with dplyr (21p, 171-193) RS2.3 Chap14 Pipes with magrittr (6p, 261-268) Readings Theory R2.1 Laube and Purves (2011): How fast is a cow? cross - scale analysis of movement data. "],["W02_03_preperation.html", "Preperation", " Preperation Install the package zoo to get access to the rolling window functions for last exercise. install.packages(&quot;zoo&quot;) Import the wild boar data and convert it to an sf object with CH1903+ LV95 Coordinates. Either run your own script from last week or the following lines to bring the data to the form we need it for today exercise. library(tidyverse) ## -- Attaching packages --------------------------------------- tidyverse 1.3.0 -- ## v tibble 3.0.4 v purrr 0.3.4 ## v tidyr 1.1.2 v forcats 0.5.1 ## Warning: package &#39;tidyr&#39; was built under R version 4.0.4 ## -- Conflicts ------------------------------------------ tidyverse_conflicts() -- ## x terra::collapse() masks dplyr::collapse(), glue::collapse() ## x terra::desc() masks dplyr::desc() ## x tidyr::expand() masks terra::expand() ## x tidyr::extract() masks terra::extract() ## x tidyr::fill() masks terra::fill() ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() ## x terra::near() masks dplyr::near() ## x tidyr::pack() masks terra::pack() ## x terra::select() masks dplyr::select() ## x tidyr::separate() masks terra::separate() ## x purrr::transpose() masks terra::transpose() library(sf) library(lubridate) ## Warning: package &#39;lubridate&#39; was built under R version 4.0.4 ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:terra&#39;: ## ## intersect, union ## The following objects are masked from &#39;package:base&#39;: ## ## date, intersect, setdiff, union wildschwein_BE &lt;- read_delim(&quot;00_Rawdata/wildschwein_BE.csv&quot;,&quot;,&quot;) ## ## -- Column specification -------------------------------------------------------- ## cols( ## TierID = col_character(), ## TierName = col_character(), ## CollarID = col_double(), ## DatetimeUTC = col_datetime(format = &quot;&quot;), ## Lat = col_double(), ## Long = col_double() ## ) wildschwein_BE = st_as_sf(wildschwein_BE, coords = c(&quot;Long&quot;, &quot;Lat&quot;), crs = 4326) wildschwein_BE &lt;- st_transform(wildschwein_BE, 2056) "],["W02_04_demo_tidyverse.html", "Demo Tidyverse", " Demo Tidyverse Depending on your knowledge of R, getting an overview of the data we imported last week might have been quite a challenge. Surprisingly enough, importing, cleaning and exploring your data can be the most challenging, time consuming part of a project. RStudio and the tidyverse offer many helpful tools to make this part easier (and more fun). You have read chapters on dplyr and magrittr as a preparation for this exercise. Before we start with the exercise however, this demo illustrates a simple approach offered by tidyverse which is applicable to sf-objects. Assume we want to calculate the timelag between subsequent positions. To achieve this we can use the function difftime() combined with lead() from dplyr. Lets look at these functions one by one. difftime difftime takes two POSIXct values. now &lt;- Sys.time() later &lt;- now + 10000 time_difference &lt;- difftime(later,now) time_difference ## Time difference of 2.777778 hours You can also specify the unit of the output. time_difference &lt;- difftime(later,now,units = &quot;mins&quot;) time_difference ## Time difference of 166.6667 mins difftime returns an object of the Class difftime. However in our case, numeric values would be more handy than the Class difftime. So well wrap the command in as.numeric(): str(time_difference) ## &#39;difftime&#39; num 166.666666666667 ## - attr(*, &quot;units&quot;)= chr &quot;mins&quot; time_difference &lt;- as.numeric(difftime(later,now,units = &quot;mins&quot;)) str(time_difference) ## num 167 lead() / lag() lead() and lag() return a vector of the same length as the input, just offset by a specific number of values (default is 1). Consider the following sequence: numbers &lt;- 1:10 numbers ## [1] 1 2 3 4 5 6 7 8 9 10 We can now run lead() and lag() on this sequence to illustrate the output. n = specifies the offset, default = specifies the default value used to fill the emerging empty spaces of the vector. lead(numbers) ## [1] 2 3 4 5 6 7 8 9 10 NA lead(numbers,n = 2) ## [1] 3 4 5 6 7 8 9 10 NA NA lag(numbers) ## [1] NA 1 2 3 4 5 6 7 8 9 lag(numbers,n = 5) ## [1] NA NA NA NA NA 1 2 3 4 5 lag(numbers,n = 5, default = 0) ## [1] 0 0 0 0 0 1 2 3 4 5 This helps us performing operations on subsequent values in a vector (or rows in a table). You can think of this a little bit like a moving temporal window that moves along the trajectory, or down the rows of a table respectively. lead(numbers)-numbers ## [1] 1 1 1 1 1 1 1 1 1 NA mutate() Using the above functions (difftime() and lead()), we can calculate the time lag, that is, the time difference between subsequent positions: wildschwein_BE$timelag &lt;- as.numeric(difftime(lead(wildschwein_BE$DatetimeUTC), wildschwein_BE$DatetimeUTC, units = &quot;secs&quot;)) We mention wildschwein_BE three times in this function, which is complicated. Instead, we can use mutate() to simplify the syntax: wildschwein_BE &lt;- mutate(wildschwein_BE,timelag = as.numeric(difftime(lead(DatetimeUTC), DatetimeUTC, units = &quot;secs&quot;))) group_by() Now lets have a look at the vector created before: summary(wildschwein_BE$timelag) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## -22648470 896 903 571 916 60367 1 These values dont make much sense: some are negative (which should not be the case) and some are very high (which would indicate large data gaps and should not be the case either). The reason for this result is that we did not consider that timelag should just be calculated between subsequent rows of the same individual. We can implement this by using group_by() (just as if calculating the convex hull last week). wildschwein_BE &lt;- group_by(wildschwein_BE,TierID) After adding this grouping variable, calculating the timelag automatically accounts for the individual trajectories. wildschwein_BE &lt;- mutate(wildschwein_BE,timelag = as.numeric(difftime(lead(DatetimeUTC), DatetimeUTC, units = &quot;secs&quot;))) summary(wildschwein_BE$timelag) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 12 896 903 1408 916 60367 3 summarise() summary() returned the metrics over all individuals. If we want to summarise our data and get metrics per animal, we can use the dplyr function summarise(). In contrast to mutate(), which just adds a new column to the dataset, summarise() collapses the data to one row per individual (specified by group_by). summarise(wildschwein_BE, mean = mean(timelag, na.rm = T)) The above operation works fine on normal data.frames, but since wildschwein_BE is also an sf object, summarise actually merges all the points to a multipoint geometry, which takes a long time to calculate. In order to prevent this, we can set the objects geometry to NULL, which removes the spatial attribute. summarise(st_set_geometry(wildschwein_BE,NULL), mean_timelag = mean(timelag, na.rm = T)) ## # A tibble: 3 x 2 ## TierID mean_timelag ## * &lt;chr&gt; &lt;dbl&gt; ## 1 002A 1286. ## 2 016A 1412. ## 3 018A 1599. Piping The code above may be a bit hard to read, since it has so many nested functions which need to be read from the inside out. In order to make code readable in a more human-friendly way, we can use the piping command %&gt;% from magrittr, which is included in dplyr and the tidyverse. The above code then looks like this: wildschwein_BE %&gt;% # Take wildschwein_BE... st_set_geometry(NULL) %&gt;% # ...remove the geometry column... group_by(TierID) %&gt;% # ...group it by TierID summarise( # Summarise the data... mean_timelag = mean(timelag,na.rm = T) # ...by calculating the mean timelag ) ## # A tibble: 3 x 2 ## TierID mean_timelag ## * &lt;chr&gt; &lt;dbl&gt; ## 1 002A 1286. ## 2 016A 1412. ## 3 018A 1599. Bring it all together Here is the same approach with a different, smaller dataset: pigs = data.frame( TierID=c(8001,8003,8004,8005,8800,8820,3000,3001,3002,3003,8330,7222), sex=c(&quot;M&quot;,&quot;M&quot;,&quot;M&quot;,&quot;F&quot;,&quot;M&quot;,&quot;M&quot;,&quot;F&quot;,&quot;F&quot;,&quot;M&quot;,&quot;F&quot;,&quot;M&quot;,&quot;F&quot;), age=c(&quot;A&quot;,&quot;A&quot;,&quot;J&quot;,&quot;A&quot;,&quot;J&quot;,&quot;J&quot;,&quot;J&quot;,&quot;A&quot;,&quot;J&quot;,&quot;J&quot;,&quot;A&quot;,&quot;A&quot;), weight=c(50.755,43.409,12.000,16.787,20.987,25.765,22.0122,21.343,12.532,54.32,11.027,88.08) ) pigs ## TierID sex age weight ## 1 8001 M A 50.7550 ## 2 8003 M A 43.4090 ## 3 8004 M J 12.0000 ## 4 8005 F A 16.7870 ## 5 8800 M J 20.9870 ## 6 8820 M J 25.7650 ## 7 3000 F J 22.0122 ## 8 3001 F A 21.3430 ## 9 3002 M J 12.5320 ## 10 3003 F J 54.3200 ## 11 8330 M A 11.0270 ## 12 7222 F A 88.0800 pigs %&gt;% summarise( mean_weight = mean(weight) ) ## mean_weight ## 1 31.58477 pigs %&gt;% group_by(sex) %&gt;% summarise( mean_weight = mean(weight) ) ## # A tibble: 2 x 2 ## sex mean_weight ## * &lt;chr&gt; &lt;dbl&gt; ## 1 F 40.5 ## 2 M 25.2 pigs %&gt;% group_by(sex,age) %&gt;% summarise( mean_weight = mean(weight) ) ## `summarise()` has grouped output by &#39;sex&#39;. You can override using the `.groups` argument. ## # A tibble: 4 x 3 ## # Groups: sex [2] ## sex age mean_weight ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 F A 42.1 ## 2 F J 38.2 ## 3 M A 35.1 ## 4 M J 17.8 "],["W02_05_tasks_and_inputs.html", "Tasks and Inputs", " Tasks and Inputs Task 1: Getting an overview Calculate the time difference between subsequent rows as described in the demo (column timelag). First, inspect your data in more detail. Try to answer the following questions: How many individuals were tracked? For how long were the individual tracked? Are there gaps? Were all individuals tracked concurrently or sequentially? What is the temporal sampling interval between the locations? Here are some exemplary visualisation you could produce to answer these questions. Can you now answer the above questions? ## Warning: Removed 35 rows containing non-finite values (stat_bin). ## Warning: Transformation introduced infinite values in continuous y-axis ## Warning: Removed 205 rows containing missing values (geom_bar). Input: Geometry as columns Last week, we transformed our data from a data.frame to an sf object. This turned our Lat/Long columns into a single geometry (list) column. While this is very handy for many spatial operations, accessing the coordinates directly becomes difficult. We therefore suggest storing the information twice, once as a geometry and once as a numeric value. To do this, we have to extract the Coordinates using st_coordinates(). We can store these values in a new variable and display them: # Store coordinates in a new variable coordinates &lt;- st_coordinates(wildschwein_BE) head(coordinates) ## X Y ## 1 2570409 1204752 ## 2 2570402 1204863 ## 3 2570394 1204826 ## 4 2570379 1204817 ## 5 2570390 1204818 ## 6 2570390 1204825 Note that that the column are named X and Y, while CH1903+ LV95 names the Axes E and N: lets rename the columns appropriately. After this, we can use cbind() to glue the columns to our original sf-object. colnames(coordinates) &lt;- c(&quot;E&quot;,&quot;N&quot;) wildschwein_BE &lt;- cbind(wildschwein_BE,coordinates) head(wildschwein_BE) ## Simple feature collection with 6 features and 7 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: 2570379 ymin: 1204752 xmax: 2570409 ymax: 1204863 ## projected CRS: CH1903+ / LV95 ## TierID TierName CollarID DatetimeUTC timelag E N ## 1 002A Sabi 12275 2014-08-22 21:00:12 904 2570409 1204752 ## 2 002A Sabi 12275 2014-08-22 21:15:16 927 2570402 1204863 ## 3 002A Sabi 12275 2014-08-22 21:30:43 924 2570394 1204826 ## 4 002A Sabi 12275 2014-08-22 21:46:07 855 2570379 1204817 ## 5 002A Sabi 12275 2014-08-22 22:00:22 888 2570390 1204818 ## 6 002A Sabi 12275 2014-08-22 22:15:10 903 2570390 1204825 ## geometry ## 1 POINT (2570409 1204752) ## 2 POINT (2570402 1204863) ## 3 POINT (2570394 1204826) ## 4 POINT (2570379 1204817) ## 5 POINT (2570390 1204818) ## 6 POINT (2570390 1204825) #- chunkend Task 2: Deriving movement parameters I: Speed In this task we will derive some additional movement parameters from our trajectories. So far our trajectories only consist of a list of time-stamped spatial locations. So lets calculate the animals steplength based on the Euclidean distance between two subsequent locations. You can calculate the Euclidean distance with the following formula: sqrt((E1-E2)^2+(N1-N2)^2) use lead(E,1) to address the the row n+1 (i.e. E2) Why do we use E and N when calculating the Euclidean distance, and not Lat/Long? Now calculate the animals speed between subsequent locations based on the steplength as calculated in the previous task and the timelag between the locations. What speed unit do you get? Task 3: Cross-scale movement analysis Laube and Purves (2011) analyse animal movement across different scales (see below). We will do the same on a subset of our data. knitr::include_graphics(&quot;02_Images/laube_2011_2.jpg&quot;) Figure 2: Black points are used in calculation of movement parameters (e.g. speed) at a given termporal scale (Laube and Purves, 2011) Import Caro60 In the first task, we saw that the animals are sampled at different frequencies. To simplify the task, weve prepared a dataset that includes 200 locations of a single wild boar with a constant sampling interval of 60 seconds. Import this dataset named caro60.csv (available on moodle) just like you imported the other wild boar data. NOTE: Weve converted the positions to CH1903+ LV95 for your convenience. Consider this when transforming to sf! Save this data to a new variable (we will use caro60). Resample Now manually reduce the granularity of our sampling interval by selecting every 3rd, 6th and 9th position. If you like to stick to the tidyverse approach, you can use slice() to subset the dataset by row number. Slice takes an integer vector. Eg: slice(dataset, 1:10), returns the first 10 rows of a dataset, slice(dataset, c(1,5,10)) returns the 1st, 5th and 10th value of a dataset. Save each re-sampled dataset in a new variable. We will use caro60_3, caro60_6 and caro60_9. You should now have 4 data sets with different number of rows: nrow(caro60) ## [1] 200 nrow(caro60_3) ## [1] 67 nrow(caro60_6) ## [1] 34 nrow(caro60_9) ## [1] 23 Update derived parameters timelag, steplength and speed now have to be recalculated for the three re-sampled data sets. Do so as we illustrated in the Chapter Demo. Visualize Compare the speeds in a line plot and visualize the trajectories in a map (see examples below). Interpret the line plot, what do the different lines for the different temporal granularities tell you? Weve stored the geographic location of our point in the trajectory in three different forms in our dataset. Once as a geometry, once as E/N and once as lat/long. In our view, it is most practical to use the E/N (integer) columns of our data to map them in this task geom_sf() does not plot lines, just points Therefore, use geom_path() and geom_point() rather than geom_sf() within ggplot In contrast to geom_sf(), you have to explicitly specify the x/y columns (in our case E/N) with geom_path()/geom_point() geom_line() does not work when mapping trajectory data, since it connects the observations in order of the variable on the x axis. geom_path() connects the observations in the order in which they appear in the data ## ## -- Column specification -------------------------------------------------------- ## cols( ## TierID = col_character(), ## TierName = col_character(), ## CollarID = col_double(), ## DatetimeUTC = col_datetime(format = &quot;&quot;), ## N = col_double(), ## E = col_double() ## ) ## Warning: Removed 1 row(s) containing missing values (geom_path). ## Warning: Removed 1 row(s) containing missing values (geom_path). ## Warning: Removed 1 row(s) containing missing values (geom_path). ## Warning: Removed 1 row(s) containing missing values (geom_path). Task 4: Deriving movement parameters II: Rolling window functions A different approach would be to smoothen the derived parameters using a moving window function. The zoo package offers a variate of moving window functions (roll_*). Use roll_mean() to smooth the calculated speed. Familiarise yourself with this function by working on some dummy data, for example: library(zoo) ## ## Attaching package: &#39;zoo&#39; ## The following object is masked from &#39;package:terra&#39;: ## ## time&lt;- ## The following objects are masked from &#39;package:base&#39;: ## ## as.Date, as.Date.numeric example &lt;- rnorm(10) rollmean(example,k = 3,fill = NA,align = &quot;left&quot;) ## [1] 0.93634335 0.31709038 0.02370048 0.67869801 0.73369105 0.50401344 ## [7] -0.56144365 -0.56902598 NA NA rollmean(example,k = 4,fill = NA,align = &quot;left&quot;) ## [1] 0.6775521 0.2045005 0.5848215 0.5255629 0.3446928 0.1459635 ## [7] -0.4102301 NA NA NA Now run rollmeanon the speed variable of the subset (caro60). Visualize the output from your moving windows and compare different window sizes (k =). Task 5 (optional): Calculate turning angles Just like we did with speed in tasks 2 - 4, we could do the same with turning angles of the trajectory. If you like a challenge, try to calculate these with the same approach! Warning: this task is pretty complex. Note, as this task is optional, you dont have to include it in your mandatory submission of Exercise 2! "],["W03_01_exercise.html", "Exercise 3", " Exercise 3 "],["W03_01_learning_outcomes.html", "Learning Outcomes", " Learning Outcomes You are able to segment a trajectory, e.g. using the approach proposed in Laube and Purves (2011) You are able to compute the similarity between given trajectories using the package SimilarityMeasures. You acquire further useful data processing skills. "],["W03_02_prerequisites.html", "Prerequisites", " Prerequisites Readings Skills from R for Data Science (Wickham and Grolemund 2017): RS3.1 Chap1 Data visualization with ggplot2 (31, 3-35) RS3.2 Chap5 Exploratory Data Analysis (28p, 81.109) Readings Theory: Alan Both (2018) A Comparative Analysis of Trajectory Similarity Measures: Recommendations for Selection and Use, excerpt from an unpublished manuscript, confidential. "],["W03_03_preperation.html", "Preperation", " Preperation Install the following libraries: ## Preperation ################################################################# install.packages(&quot;SimilarityMeasures&quot;) # The following packages are for optional tasks: install.packages(&quot;plotly&quot;) # You don&#39;t really need the following packages, # we just use them in our figures install.packages(&quot;ggrepel&quot;) Open your R Project from last week. Either run your own script from last week or the following lines to transform the data into the form we need for todays exercise. library(tidyverse) library(sf) # Import as dataframe wildschwein_BE &lt;- read_delim(&quot;00_Rawdata/wildschwein_BE.csv&quot;,&quot;,&quot;) ## ## -- Column specification -------------------------------------------------------- ## cols( ## TierID = col_character(), ## TierName = col_character(), ## CollarID = col_double(), ## DatetimeUTC = col_datetime(format = &quot;&quot;), ## Lat = col_double(), ## Long = col_double() ## ) # Convert to sf-object wildschwein_BE = st_as_sf(wildschwein_BE, coords = c(&quot;Long&quot;, &quot;Lat&quot;), crs = 4326,remove = FALSE) # transform to CH1903 LV95 wildschwein_BE &lt;- st_transform(wildschwein_BE, 2056) # Add geometry as E/N integer Columns wildschwein_BE &lt;- st_coordinates(wildschwein_BE) %&gt;% cbind(wildschwein_BE,.) %&gt;% rename(E = X) %&gt;% rename(N = Y) # Compute timelag, steplength and speed wildschwein_BE &lt;- wildschwein_BE %&gt;% group_by(TierID) %&gt;% mutate( timelag = as.numeric(difftime(lead(DatetimeUTC),DatetimeUTC,units = &quot;secs&quot;)), steplength = sqrt((E-lead(E))^2+(N-lead(N))^2), speed = steplength/timelag ) "],["W03_04_tasks_and_inputs.html", "Tasks and Inputs", " Tasks and Inputs Input: Segmentation as in Laube and Purves (2011) Youve read Laube and Purves (2011) about segmenting trajectories. In the paper, the authors define static fixes as those whose average Euclidean distance to other fixes inside a temporal window v is less than some threshold d, as illustrated in the following figure: The above image from Laube and Purves (2011) visualizes the following steps: Temporal representation of constant sample interval with associated temporal window v for three exemplary points; Measurement of average distance in temporal window v to sample points in spatial representation; Removal of all points where average distance is less than a given threshold, i.e. removal of static points; and Removal of subtrajectories with less than a threshold temporal length. We will implement this method on the following dummy data. Once youve grasped the idea on this simple data, you will implement it for the wild boar data in task 1. Note: I use tibble() instead of data.frame(). The two functions are very similar, this is just a matter of preference. set.seed(10) n = 20 df &lt;- tibble(X = cumsum(rnorm(n)), Y = cumsum(rnorm(n))) ggplot(df, aes(X,Y)) + geom_path() + geom_point() + coord_equal() Segmenting The first step is calculating the distances to temporally close samples within the temporal window v. Take the following sample data, assuming the sampling interval is 5 minutes. If we take a temporal window of 20 minutes, that would mean including 5 fixes. We need to calculate the following Euclidean distances (pos representing single location): pos[n-2] to pos[n] pos[n-1] to pos[n] pos[n] to pos[n+1] pos[n] to pos[n+2] Just like last week, we use the formular for calculating the Euclidean distance in in combination with lead() and lag(). For example, to create the necessary offset of n-2, we use lag(x, 2). For each offset, we create one individual column. df &lt;- df %&gt;% mutate( nMinus2 = sqrt((lag(X,2)-X)^2+(lag(Y,2)-Y)^2), # distance to pos -10 minutes nMinus1 = sqrt((lag(X,1)-X)^2+(lag(Y,1)-Y)^2), # distance to pos - 5 minutes nPlus1 = sqrt((X-lead(X,1))^2+(Y-lead(Y,1))^2), # distance to pos + 5 mintues nPlus2 = sqrt((X-lead(X,2))^2+(Y-lead(Y,2))^2) # distance to pos +10 minutes ) Now we want to calculate the mean distance of nMinus2, nMinus1, nPlus1, nPlus2 for each row. The below function calculates the overall mean of all columns, which is not what we want. df %&gt;% mutate( stepMean = mean(c(nMinus2, nMinus1,nPlus1,nPlus2), na.rm = T) ) ## # A tibble: 20 x 7 ## X Y nMinus2 nMinus1 nPlus1 nPlus2 stepMean ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0187 -0.596 NA NA 2.19 3.26 1.72 ## 2 -0.166 -2.78 NA 2.19 1.53 3.42 1.72 ## 3 -1.54 -3.46 3.26 1.53 2.20 3.40 1.72 ## 4 -2.14 -5.58 3.42 2.20 1.30 1.78 1.72 ## 5 -1.84 -6.84 3.40 1.30 0.540 1.34 1.72 ## 6 -1.45 -7.21 1.78 0.540 1.39 2.21 1.72 ## 7 -2.66 -7.90 1.34 1.39 0.945 2.22 1.72 ## 8 -3.02 -8.77 2.21 0.945 1.63 1.92 1.72 ## 9 -4.65 -8.88 2.22 1.63 0.361 2.27 1.72 ## 10 -4.91 -9.13 1.92 0.361 2.16 2.68 1.72 ## 11 -3.80 -11.0 2.27 2.16 0.760 1.03 1.72 ## 12 -3.05 -11.1 2.68 0.760 0.997 1.38 1.72 ## 13 -3.29 -10.1 1.03 0.997 1.00 2.10 1.72 ## 14 -2.30 -9.91 1.38 1.00 1.57 2.94 1.72 ## 15 -1.56 -11.3 2.10 1.57 1.44 1.38 1.72 ## 16 -1.47 -12.7 2.94 1.44 1.02 1.81 1.72 ## 17 -2.42 -12.4 1.38 1.02 1.77 2.21 1.72 ## 18 -2.62 -14.1 1.81 1.77 0.981 1.71 1.72 ## 19 -1.69 -14.4 2.21 0.981 0.811 NA 1.72 ## 20 -1.21 -15.1 1.71 0.811 NA NA 1.72 Since we want the mean value per Row, we have to explicitly specify this before mutate() with the function rowwise(). Note the new grouping variable &lt;by row&gt; when printing the dataframe to the console. df &lt;- df %&gt;% rowwise() %&gt;% mutate( stepMean = mean(c(nMinus2, nMinus1,nPlus1,nPlus2)) ) df ## # A tibble: 20 x 7 ## # Rowwise: ## X Y nMinus2 nMinus1 nPlus1 nPlus2 stepMean ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0187 -0.596 NA NA 2.19 3.26 NA ## 2 -0.166 -2.78 NA 2.19 1.53 3.42 NA ## 3 -1.54 -3.46 3.26 1.53 2.20 3.40 2.60 ## 4 -2.14 -5.58 3.42 2.20 1.30 1.78 2.17 ## 5 -1.84 -6.84 3.40 1.30 0.540 1.34 1.64 ## 6 -1.45 -7.21 1.78 0.540 1.39 2.21 1.48 ## 7 -2.66 -7.90 1.34 1.39 0.945 2.22 1.47 ## 8 -3.02 -8.77 2.21 0.945 1.63 1.92 1.68 ## 9 -4.65 -8.88 2.22 1.63 0.361 2.27 1.62 ## 10 -4.91 -9.13 1.92 0.361 2.16 2.68 1.78 ## 11 -3.80 -11.0 2.27 2.16 0.760 1.03 1.55 ## 12 -3.05 -11.1 2.68 0.760 0.997 1.38 1.45 ## 13 -3.29 -10.1 1.03 0.997 1.00 2.10 1.28 ## 14 -2.30 -9.91 1.38 1.00 1.57 2.94 1.72 ## 15 -1.56 -11.3 2.10 1.57 1.44 1.38 1.62 ## 16 -1.47 -12.7 2.94 1.44 1.02 1.81 1.80 ## 17 -2.42 -12.4 1.38 1.02 1.77 2.21 1.59 ## 18 -2.62 -14.1 1.81 1.77 0.981 1.71 1.57 ## 19 -1.69 -14.4 2.21 0.981 0.811 NA NA ## 20 -1.21 -15.1 1.71 0.811 NA NA NA We can now determin if an animal is moving or not by specifying a threshold on stepMean df &lt;- df %&gt;% mutate( moving = stepMean&gt;1.5 ) df ## # A tibble: 20 x 8 ## # Rowwise: ## X Y nMinus2 nMinus1 nPlus1 nPlus2 stepMean moving ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 0.0187 -0.596 NA NA 2.19 3.26 NA NA ## 2 -0.166 -2.78 NA 2.19 1.53 3.42 NA NA ## 3 -1.54 -3.46 3.26 1.53 2.20 3.40 2.60 TRUE ## 4 -2.14 -5.58 3.42 2.20 1.30 1.78 2.17 TRUE ## 5 -1.84 -6.84 3.40 1.30 0.540 1.34 1.64 TRUE ## 6 -1.45 -7.21 1.78 0.540 1.39 2.21 1.48 FALSE ## 7 -2.66 -7.90 1.34 1.39 0.945 2.22 1.47 FALSE ## 8 -3.02 -8.77 2.21 0.945 1.63 1.92 1.68 TRUE ## 9 -4.65 -8.88 2.22 1.63 0.361 2.27 1.62 TRUE ## 10 -4.91 -9.13 1.92 0.361 2.16 2.68 1.78 TRUE ## 11 -3.80 -11.0 2.27 2.16 0.760 1.03 1.55 TRUE ## 12 -3.05 -11.1 2.68 0.760 0.997 1.38 1.45 FALSE ## 13 -3.29 -10.1 1.03 0.997 1.00 2.10 1.28 FALSE ## 14 -2.30 -9.91 1.38 1.00 1.57 2.94 1.72 TRUE ## 15 -1.56 -11.3 2.10 1.57 1.44 1.38 1.62 TRUE ## 16 -1.47 -12.7 2.94 1.44 1.02 1.81 1.80 TRUE ## 17 -2.42 -12.4 1.38 1.02 1.77 2.21 1.59 TRUE ## 18 -2.62 -14.1 1.81 1.77 0.981 1.71 1.57 TRUE ## 19 -1.69 -14.4 2.21 0.981 0.811 NA NA NA ## 20 -1.21 -15.1 1.71 0.811 NA NA NA NA ggplot(df, aes(X,Y)) + geom_path() + geom_point(aes(colour = moving)) + coord_equal() Unique IDs per segment When segmenting trajectories, we often want to compute metrics on the basis of each segment. Within the tidyverse logic, we need a unique ID per segment that we can pass to group_by(). In other words, we need a unique ID for a sequence of successive TRUE values. For lack of a better way, we suggest solving this problem with cumsum(). cumsum() returns the cummulative sum of a given vector: one_to_ten &lt;- 1:10 one_to_ten ## [1] 1 2 3 4 5 6 7 8 9 10 cumsum(one_to_ten) ## [1] 1 3 6 10 15 21 28 36 45 55 In R, TRUEand FALSE are interprated as 1 and 0 if coerced to an integer. as.integer(TRUE) ## [1] 1 as.integer(FALSE) ## [1] 0 TRUE+TRUE ## [1] 2 Therefore, cumsum() on a boolean vector increases the count on each TRUE value: boolvec &lt;- c(FALSE,TRUE,TRUE,TRUE,FALSE,FALSE,TRUE,TRUE) df_cumsum &lt;- tibble(boolvec = boolvec,cumsum = cumsum(boolvec)) df_cumsum ## # A tibble: 8 x 2 ## boolvec cumsum ## &lt;lgl&gt; &lt;int&gt; ## 1 FALSE 0 ## 2 TRUE 1 ## 3 TRUE 2 ## 4 TRUE 3 ## 5 FALSE 3 ## 6 FALSE 3 ## 7 TRUE 4 ## 8 TRUE 5 You might have noticed that this is pretty much exactly the opposite of what we need. We therefore have to take the inverse of the boolean vector: df_cumsum %&gt;% mutate( boolvec_inverse = !boolvec, cumsum2 = cumsum(boolvec_inverse) ) ## # A tibble: 8 x 4 ## boolvec cumsum boolvec_inverse cumsum2 ## &lt;lgl&gt; &lt;int&gt; &lt;lgl&gt; &lt;int&gt; ## 1 FALSE 0 TRUE 1 ## 2 TRUE 1 FALSE 1 ## 3 TRUE 2 FALSE 1 ## 4 TRUE 3 FALSE 1 ## 5 FALSE 3 TRUE 2 ## 6 FALSE 3 TRUE 3 ## 7 TRUE 4 FALSE 3 ## 8 TRUE 5 FALSE 3 To simplify our workflow, we can we can take the inverse of boolvec within our cumsum() statement and save an extra line of code. df_cumsum %&gt;% mutate( cumsum2 = cumsum(!boolvec) ) ## # A tibble: 8 x 3 ## boolvec cumsum cumsum2 ## &lt;lgl&gt; &lt;int&gt; &lt;int&gt; ## 1 FALSE 0 1 ## 2 TRUE 1 1 ## 3 TRUE 2 1 ## 4 TRUE 3 1 ## 5 FALSE 3 2 ## 6 FALSE 3 3 ## 7 TRUE 4 3 ## 8 TRUE 5 3 #- chunkend Task 1: Segmentation With the skills from the input above we can now implement the segmentation algorithm described in Laube and Purves (2011). The described method depends on a regular sampling interval. Therefore, take the dataset caro60.csv from task 3 of last week (available on moodle). Import it as a dataframe, we dont need an sf-object to for todays tasks. Next, we have to have to define our temporal window v (Laube and Purves 2011). To keep things simple, I would suggest a window of n +/- 2 minutes. With a sampling interval of around 1 minute, this corresponds to a window size of n +/- 2 positions. #- chunkend Task 2: Specify and apply threshold d After calculating the Euclidean distances to positions within the temporal window v in task 1, you can explore these values (we stored them in the column stepMean) using summary statistics (histograms, boxplot, summary()): This way we can define a reasonable threshold value to differentiate between stops and moves. There is no correct way of doing this, specifying a threshold always depends on data as well as the question that needs to be answered. In this exercise, find a threshold that matches your intuition. Store the new information (boolean to differentiate between stops (TRUE) and moves (FALSE)) in a new column named moving. ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 0.8109 2.2397 3.3496 5.0371 5.8618 24.2903 4 ## Warning: Removed 4 rows containing non-finite values (stat_bin). Task 3: Visualize segmented trajectories Now visualize the segmented trajectory spatially. Just like last week, you can use ggplot with geom_path(), geom_point() and coord_equal(). Assign colour = moving within aes() to distinguish between segments with movement and without. ## Warning: package &#39;plotly&#39; was built under R version 4.0.4 ## ## Attaching package: &#39;plotly&#39; ## The following object is masked from &#39;package:terra&#39;: ## ## select ## The following object is masked from &#39;package:ggplot2&#39;: ## ## last_plot ## The following object is masked from &#39;package:stats&#39;: ## ## filter ## The following object is masked from &#39;package:graphics&#39;: ## ## layout Tip: if you want to get a ggplot() with Zoom capability, just load the library plotly and wrap the ggplot() function with ggplotly() Task 4: Segment-based analysis In applying Laube and Purves (2011), weve come as far as steps (b)/(c) in Figure 1. In order to complete step (d) (Removal of subtrajectories with less than a threshold temporal length), we have to calculate each segments temporal duration. In order to do this, we need a unique name for each segment that we can use as a grouping variable. This is where the cumsum() approach which we introduced in the input comes useful. Complete the following steps: Filter the data by removing all rows where moving equals to NA (typically these are the the first and last 2 rows). Get a unique ID per segment and using the cumsum() approach we introduced in the input. Store this ID in a new column segment_ID Filter the data to remove all rows where the animal is not moving Group by the column segment_ID to calculate the temporal length of each segment with mutate(). Remove segments with a duration less than 3 minutes inspect your data visually Task 5: Similarity measures Import the dataset pedestrian.csv (available on moodle) as a dataframe (you dont need an sf object). It it a set of six different but similar trajectories from pedestrians walking on a path. Explore this data visually. We will analyse these trajectories with the package SimilarityMeasures, always comparing trajectory 1 pairwise to the other trajectories 2-6. For this task, explore the trajectories first and get an idea on how the pedestrians moved. We step away from using the wild boar data for this task because our animals dont express the type of similarity we want to illustrate here. Also, using the constructed pedestrian data allows us illustrating very typical similarity issues, that are picked-up in different ways by the different similarity measures. In later exercises we will get back to our wild boar! ## ## -- Column specification -------------------------------------------------------- ## cols( ## TrajID = col_double(), ## E = col_double(), ## N = col_double(), ## DatetimeUTC = col_datetime(format = &quot;&quot;) ## ) ## Warning: ggrepel: 1 unlabeled data points (too many overlaps). Consider ## increasing max.overlaps #- chunkend Task 6: Calculate similarity Calculate the similarity between trajectory 1 and trajectories 2-6. Use the different similarity measures in the package SimilarityMeasures. Visualize your results and try to understand the different results with respect to your reading of Alan Both (2018). Can you see connections between the properties of the trajectories and the similarity values computed by the different measures? Note: All functions in the package need matrices as input, with one trajectory per matrix. LCSStakes very long to compute. The accuracy of the algorithm (pointSpacing = ,pointDistance = and errorMarg =) can be varied to provide faster calculations. Please see Vlachos, Gunopoulos, and Kollios (2002) for more information. ## Warning: `data_frame()` was deprecated in tibble 1.1.0. ## Please use `tibble()` instead. ## Warning: attributes are not identical across measure variables; ## they will be dropped "],["W04_01_exercise.html", "Exercise 4", " Exercise 4 "],["W04_01_learning_outcomes.html", "Learning Outcomes", " Learning Outcomes You are able to conceptualize a simple movement pattern and implement data structures and corresponding procedures (lets call them algorithms) for detecting it using R. You understand the sensitivity of movement patterns to pattern parameter thresholds. "],["W04_02_prerequisites.html", "Prerequisites", " Prerequisites Readings Skills from R for Data Science (Wickham and Grolemund 2017): RS4.1 Chap15 Functions (19p, 269-289) Readings Theory, Laube (2014) - R4.1 Chap.2, p. 29-58 "],["W04_03_preparation.html", "Preparation", " Preparation ## Preperation ################################################################# Open your R Project from last week. Either run your own script from last week or the following lines to transform the data into the form we need for todays exercise. library(tidyverse) library(sf) library(lubridate) # Import as tibble wildschwein_BE &lt;- read_delim(&quot;00_Rawdata/wildschwein_BE.csv&quot;,&quot;,&quot;) ## ## -- Column specification -------------------------------------------------------- ## cols( ## TierID = col_character(), ## TierName = col_character(), ## CollarID = col_double(), ## DatetimeUTC = col_datetime(format = &quot;&quot;), ## Lat = col_double(), ## Long = col_double() ## ) # Convert to sf-object wildschwein_BE = st_as_sf(wildschwein_BE, coords = c(&quot;Long&quot;, &quot;Lat&quot;), crs = 4326,remove = FALSE) # transform to CH1903 LV95 wildschwein_BE &lt;- st_transform(wildschwein_BE, 2056) # Add geometry as E/N integer Columns wildschwein_BE &lt;- st_coordinates(wildschwein_BE) %&gt;% cbind(wildschwein_BE,.) %&gt;% rename(E = X) %&gt;% rename(N = Y) # Compute timelag, steplength and speed wildschwein_BE &lt;- wildschwein_BE %&gt;% group_by(TierID) %&gt;% mutate( timelag = as.numeric(difftime(lead(DatetimeUTC),DatetimeUTC,units = &quot;secs&quot;)), steplength = sqrt((E-lead(E,1))^2+(N-lead(N,1))^2), speed = steplength/timelag ) #- chunkend "],["W04_04_tasks_and_inputs.html", "Tasks and inputs", " Tasks and inputs Input: Creating functions ## Input: Creating Functions ################################################### Up to now, we have used a variety of different functions designed by other developers. Sometimes we need to execute an operation multiple times, and most often it is reasonable to write a function to do so. Whenever youve copied and pasted a block of code more than twice, you should consider writing a function (Wickham and Grolemund 2017). We have violated this rule multiple times when calculating the Euclidean distances between points. Writing and rewriting the code sqrt((x-lead(x,1))^2+(y-lead(y,1))^2) over and over again is not only cumbersome, it is also error prone. We can easily wrap this operation into a function. This input on writing functions should bring you up to speed to do this in your first task. The first step in writing a function, is picking a name and assigning &lt;- function(){} to it. testfun &lt;- function(){} To run the function, we have to call the assigned name with the brackets. This function gives no output, which is why we get NULL back. testfun() ## NULL class(testfun) ## [1] &quot;function&quot; To make the function actually do something, we need to specify what should be done within the curly brackets {}. The following function always prints the same statement and accepts no input values: testfun &lt;- function(){print(&quot;this function does nothing&quot;)} testfun() ## [1] &quot;this function does nothing&quot; If we want the function to accept some input values, we have to define them within the round brackets. For example, I specify a variable named sometext and can call this variable within the execution. testfun &lt;- function(sometext){print(sometext)} testfun(sometext = &quot;this function does slightly more, but still not much&quot;) ## [1] &quot;this function does slightly more, but still not much&quot; Lets take a more practical example. Say we want a function that calculates the nth root of a value, since the base function sqrt() just returns the 2nd root. To do this, we need to know two things: taking the nth rooth of x is the same as raising x by the the reciprocal value (Kehrwert) of n \\[\\sqrt[n]{x}=\\ x^{\\frac{1}{n}}\\] We can raise a value x to the power of a value n in R with the following code: x^n To create a function that simplifies taking the nth root, we just need one line of code: # specify two parameters: # x: the value with want to take the root from # n: the root we want to take (2 for 2nd root) nthroot &lt;- function(x,n){x^(1/n)} # Test function by taking the second root of 4. # Expecting the result to be 2: nthroot(x = 4,n = 2) ## [1] 2 As we already know from using other functions, if we declare our variables in the order that we initially listed them, we do not need to specify the parameters (no need of x =and n =). nthroot(27,3) ## [1] 3 nthroot(3,3) ## [1] 1.44225 If we want any of our parameters to have default value, we can assign an initial value to the parameter when declaring the variables within the round brackets. nthroot &lt;- function(x,n = 2){x^(1/n)} # if not stated otherwise, our function takes the square root nthroot(10) ## [1] 3.162278 # We can still overwrite n nthroot(10,3) ## [1] 2.154435 All you need to do now is run these few lines of code at the beginning of your script, and you can use the function for your entire R session. After starting a new session, you will simply have to re-run the lines. So it might be a good idea to place this function within the section Loading environment / libraries of your project. Task 1: Write your own functions Create a function for our Euclidean distance calculation. Optionally, write another function to number successive TRUE values with the cumsum() approach from last week. Note: if you treat your input variables as vectors, they will work in most use cases (in particular in dplyrs mutate() and summarise() functions). Task 2: Filter data We propose conceptualizing the pattern meet as being close in space and time, with the notion close to be defined for the spatial and the temporal case separately. We will simplify the problem slightly so that we can use a number of R tools and data structures you now have learned to use by now. As a first simplification we propose manipulating the timestamps in such a way, that all observations are sampled concurrently, synchronously. This allows us using the data science concept join for detecting the temporal expression of meet - using DateTimeUTC as the key variable in a join statement: Observations with an identical time stamp will match. Once we have identified the temporal matches, we check if the concurrent observations are also close in space based on the Euclidean distances between concurrent positions. Simplifying the task even further, we will focus on an interval where we have a continuous, small sampling interval over all our animals. So, filter your dataset to the time period 01.04.2015 - 15.04.2015) and save it to a new variable (e.g. wildschwein_filter). After filtering, visualize your data spatially. Just consider animals with a spatial overlap and remove animals where we cannot expect to find any meet patterns. #- chunkend Task 3: Create Join Key head(wildschwein_filter) ## Simple feature collection with 6 features and 11 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: 2570245 ymin: 1205259 xmax: 2570375 ymax: 1205320 ## projected CRS: CH1903+ / LV95 ## # A tibble: 6 x 12 ## # Groups: TierID [1] ## TierID TierName CollarID DatetimeUTC Lat Long E N ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 002A Sabi 12275 2015-03-31 22:00:28 47.0 7.05 2570296. 1205283. ## 2 002A Sabi 12275 2015-03-31 22:15:44 47.0 7.05 2570259. 1205259. ## 3 002A Sabi 12275 2015-03-31 22:30:44 47.0 7.05 2570255. 1205259. ## 4 002A Sabi 12275 2015-03-31 22:46:04 47.0 7.05 2570245. 1205268. ## 5 002A Sabi 12275 2015-03-31 23:00:17 47.0 7.05 2570364. 1205314. ## 6 002A Sabi 12275 2015-03-31 23:15:12 47.0 7.05 2570375. 1205320. ## # ... with 4 more variables: geometry &lt;POINT [m]&gt;, timelag &lt;dbl&gt;, ## # steplength &lt;dbl&gt;, speed &lt;dbl&gt; Have a look at your dataset. You will notice that samples are taken at every full hour, quarter past, half past and quarter to. The sampling time is usually off by a couple of seconds. Verify if we have the same sampling interval (timelag) throughout our filtered dataset. ## Simple feature collection with 6 features and 11 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: 2570245 ymin: 1205259 xmax: 2570375 ymax: 1205320 ## projected CRS: CH1903+ / LV95 ## # A tibble: 6 x 12 ## # Groups: TierID [1] ## TierID TierName CollarID DatetimeUTC Lat Long E N ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 002A Sabi 12275 2015-03-31 22:00:28 47.0 7.05 2570296. 1205283. ## 2 002A Sabi 12275 2015-03-31 22:15:44 47.0 7.05 2570259. 1205259. ## 3 002A Sabi 12275 2015-03-31 22:30:44 47.0 7.05 2570255. 1205259. ## 4 002A Sabi 12275 2015-03-31 22:46:04 47.0 7.05 2570245. 1205268. ## 5 002A Sabi 12275 2015-03-31 23:00:17 47.0 7.05 2570364. 1205314. ## 6 002A Sabi 12275 2015-03-31 23:15:12 47.0 7.05 2570375. 1205320. ## # ... with 4 more variables: geometry &lt;POINT [m]&gt;, timelag &lt;dbl&gt;, ## # steplength &lt;dbl&gt;, speed &lt;dbl&gt; With a few exceptions, the sampling interval is around 15 minutes. In order to join the data, however, we need identical time stamps to serve as a join key. We therefore need to slightly adjust our time stamps to a common, concurrent interval. Round the minutes of DatetimeUTC to a multiple of 15 (00, 15, 30,45) and store the values in a new column. You can use the lubridate function round_date() for this. See the examples here to see how this goes. Please note: We are manipulating our time stamps without adjusting the x,y-coordinates. This is fine for our simple example, but we would advice against this in a more serious research endeavour, e.g. in your semester projects. One simple approach would be to linearly interpolate the positions to the new timestamps. If you choose Option A the wild boar projects as your semester projects, you should aim for a linear interpolation. Get in touch if you need help with this. Task 4: Measuring distance at concurrent locations To measure the distance between concurrent locations, we need to follow the following steps. First, split the wildschwein_filter object into one data.frame per animal.3 Next, join these datasets by the new, manipulated Datetime column and save it to the variable wildschwein_join. Which join-type is appropriate? The joined observations are temporally close. On wildschwein_join, calculate Euclidean distances between concurrent observations. Store the values in a new column distance. Use a reasonable threshold on distance to determine if the animals are also spatially close enough to constitute a meet (we use 50 meters). Store this Boolean information (TRUE/FALSE) in a new column named meet. ## # A tibble: 1,344 x 9 ## TierID.x DatetimeRound E.x N.x TierID.y E.y N.y distance ## &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 002A 2015-03-31 22:00:00 2.57e6 1.21e6 016A 2.57e6 1.20e6 482. ## 2 002A 2015-03-31 22:15:00 2.57e6 1.21e6 016A 2.57e6 1.20e6 536. ## 3 002A 2015-03-31 22:30:00 2.57e6 1.21e6 016A 2.57e6 1.20e6 542. ## 4 002A 2015-03-31 22:45:00 2.57e6 1.21e6 016A 2.57e6 1.20e6 564. ## 5 002A 2015-03-31 23:00:00 2.57e6 1.21e6 016A 2.57e6 1.20e6 486. ## 6 002A 2015-03-31 23:15:00 2.57e6 1.21e6 016A 2.57e6 1.20e6 470. ## 7 002A 2015-03-31 23:30:00 2.57e6 1.21e6 016A 2.57e6 1.20e6 440. ## 8 002A 2015-03-31 23:45:00 2.57e6 1.21e6 016A 2.57e6 1.20e6 432. ## 9 002A 2015-04-01 00:00:00 2.57e6 1.21e6 016A 2.57e6 1.20e6 451. ## 10 002A 2015-04-01 00:15:00 2.57e6 1.21e6 016A 2.57e6 1.20e6 522. ## # ... with 1,334 more rows, and 1 more variable: meet &lt;lgl&gt; Task 5: Visualize data Now, visualize the meets spatially in a way that you think reasonable. You can attempt to recreate the plot below, but this is not trivial. If you want to try it, these are the steps to take: Create a new dataset (e.g. wildschwein_meet) as follows: Using the cumsum()approach from last week, create a new column (e.g. meet_seq) enumerating the meets with individual ID (see proposed function below) Filter all rows where the animals did meet. Optional: create a column with the start- and endtime of each meet (e.g. meet_time) To create a plot similar to the one below: Initiate a ggplot() with the wildschwein_join dataset. Add two geom_point-layers (not geom_sf, since we havent turned wildschwein_join into an sfobject), one referring to E.x/N.x, the other to E.y/N.y Add two more geom_point-layers, this time with the dataset wildschwein_meet Facet the plot into small multiples either using meet_seq or meet_time ## Linking to liblwgeom 3.0.0beta1 r16016, GEOS 3.8.0, PROJ 6.3.1 Task 6 (optional): Visualize data as timecube with plotly ## Task 6 ###################################################################### Finally, you can nicely visualize the meeting patterns and trajectories in a Space-Time-Cube (Hägerstraand 1970) with the package plotly. There are some nice ressources available online. This is a perfect opportunity to learn functional programming if you are an intermediate to advanced programmer (or want to be). You can use purrr::map() specifically for these tasks. Ask us if you want to learn this but are struggling "],["W05_01_exercise.html", "Exercise 5", " Exercise 5 "],["W05_01_learning_outcomes.html", "Learning Outcomes", " Learning Outcomes You are able to process spatial data (vector and raster) within R, including creating simple maps within R. You know basic operations for semantically annotating your trajectories with geographic context. "],["W05_02_preperation.html", "Preperation", " Preperation ## Preperation ################################################################## Open your R Project from last week. Either run your own script from last week or the following lines to transform the data into the form we need for todays exercise. library(tidyverse) library(sf) library(ggspatial) ## Warning: package &#39;ggspatial&#39; was built under R version 4.0.4 library(raster) ## Loading required package: sp ## ## Attaching package: &#39;raster&#39; ## The following object is masked from &#39;package:plotly&#39;: ## ## select ## The following object is masked from &#39;package:tidyr&#39;: ## ## extract ## The following object is masked from &#39;package:dplyr&#39;: ## ## select ## The following object is masked from &#39;package:glue&#39;: ## ## trim # Import as tibble wildschwein_BE &lt;- read_delim(&quot;00_Rawdata/wildschwein_BE.csv&quot;,&quot;,&quot;) ## ## -- Column specification -------------------------------------------------------- ## cols( ## TierID = col_character(), ## TierName = col_character(), ## CollarID = col_double(), ## DatetimeUTC = col_datetime(format = &quot;&quot;), ## Lat = col_double(), ## Long = col_double() ## ) # Convert to sf-object wildschwein_BE = st_as_sf(wildschwein_BE, coords = c(&quot;Long&quot;, &quot;Lat&quot;), crs = 4326,remove = FALSE) # transform to CH1903 LV95 wildschwein_BE &lt;- st_transform(wildschwein_BE, 2056) # Add geometry as E/N integer Columns wildschwein_BE &lt;- st_coordinates(wildschwein_BE) %&gt;% cbind(wildschwein_BE,.) %&gt;% rename(E = X) %&gt;% rename(N = Y) "],["W05_03_tasks_and_inputs.html", "Tasks", " Tasks Tasks 1: Import and visualize spatial data ## Task 1 ###################################################################### Import the vector dataset Feldaufnahmen_Fanel_2016.shp from moodle with read_sf() and save it to the variable fanel2016. The file .shp stands for Shapefile, which is a simple format for spatial vector data (points, lines, or polygons). This shapefile contains vector data about cultivated crops in the study area. Transform the coordinates to CH1903+ LV95. Filter the dataset wildschwein_BE to the months May, June and July of 2016 and store the data in a new variable (wildschwein_BE_2015). Create a minimum convex polygon for each individual in wildschwein_BE_2015 and store it in a new variable (mcp2015) Create a map with the layers fanel2016 and mcp2015. ## [1] &quot;2015-07-27 11:00:14 UTC&quot; ## Warning: Number of levels of the variable &quot;Frucht&quot; is 43, which is ## larger than max.categories (which is 30), so levels are combined. Set ## tmap_options(max.categories = 43) in the layer function to show all levels. #- chunkend Task 2: Annotate Trajectories from vector data ## Task 2 ###################################################################### Now we would like to know what crop was most visited by our wild boar, and at what time. To this end, use st_join() to attach the attributes from fanel2016 to your trajectory data wildschwein_BE (semantic annotation). Visualize the number of sample points in each category of Frucht over the course of the filtered time period. ## `summarise()` has grouped output by &#39;TierID&#39;, &#39;week&#39;. You can override using the `.groups` argument. ## geom_path: Each group consists of only one observation. Do you need to adjust ## the group aesthetic? ## geom_path: Each group consists of only one observation. Do you need to adjust ## the group aesthetic? Task 3: Explore annotated trajectories Think of other ways you could visually explore the spatio-temporal patterns of wild boar in relation to the crops. Ideas: For example, in the visualisation above, we did not account for the different availability of the different crops. Potatoes (Kartoffeln) are seemingly not visited at all, while rapeseed (Raps) has high visitation from May to June. Maybe this is due to the fact that there are mostly rapeseed fields and basically no potatoe fields. How could you consider availability in the visulisation? Exlpore the circadian rhythm / daily patterns of crop visitations. Task 4: Annotate Trajectories from raster data ## Task 4 ###################################################################### In terms of raster data, we have prepared the Vegetation Height Model provided by the Swiss National Forrest Inventory (NFI). This dataset contains high resolution information (1x1 Meter) on the vegetation height, which is determined from the difference between the digital surface models DSM and the digital terrain model by swisstopo (swissAlti3D). Buildings are eliminated using a combination of the ground areas of the swisstopo topographic landscape model (TLM) and spectral information from the stereo aerial photos. ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;, prefer_proj ## = prefer_proj): Discarded datum Unknown based on Bessel 1841 ellipsoid in Proj4 ## definition ## Warning in showSRID(uprojargs, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;, prefer_proj ## = prefer_proj): Discarded datum Unknown based on Bessel 1841 ellipsoid in Proj4 ## definition ## Warning in showSRID(SRS_string, format = &quot;PROJ&quot;, multiline = &quot;NO&quot;, prefer_proj = ## prefer_proj): Discarded datum CH1903+ in Proj4 definition ## `summarise()` has grouped output by &#39;TierID&#39;. You can override using the `.groups` argument. "],["references.html", "References", " References Alan Both, Matt Duckham, Kevin Buchin. 2018. A Comparative Analysis of Trajectory Similarity Measures: Recommendations for Selection and Use. Hägerstraand, Torsten. 1970. What about People in Regional Science? Papers in Regional Science 24 (1): 724. Laube, Patrick. 2014. Computational Movement Analysis. SpringerBriefs in Computer Science. Springer International Publishing. https://books.google.ch/books?id=xMRpBAAAQBAJ. Laube, Patrick, and Ross S. Purves. 2011. How Fast Is a Cow? Cross - Scale Analysis of Movement Data. Transactions in GIS 15 (3): 40118. https://doi.org/10.1111/j.1467-9671.2011.01256.x. Vlachos, Michail, Dimitrios Gunopoulos, and George Kollios. 2002. Discovering Similar Multidimensional Trajectories. In Proceedings of the 18th International Conference on Data Engineering, 67373. ICDE 02. Washington, DC, USA: IEEE Computer Society. http://dl.acm.org/citation.cfm?id=876875.878994. Wickham, Hadley, and Garrett Grolemund. 2017. R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. 1st ed. OReilly Media, Inc. "]]
